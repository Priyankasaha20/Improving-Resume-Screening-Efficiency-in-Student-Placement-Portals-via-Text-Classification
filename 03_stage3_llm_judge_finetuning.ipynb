{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5173a06b",
      "metadata": {
        "id": "5173a06b"
      },
      "source": [
        "# 03 - Stage 3: LLM Judge with LoRA Fine-Tuning\n",
        "\n",
        "## Overview\n",
        "This notebook implements Stage 3 - the final scoring layer with explainability:\n",
        "- **Stage 3: LLM Judge with Structured Outputs**\n",
        "- Load Llama-3.2-1B or Mistral-7B with 4-bit quantization\n",
        "- Fine-tune with LoRA for resume-JD matching\n",
        "- Generate structured JSON outputs with scores + explanations\n",
        "- Provide human-readable justifications for hiring decisions\n",
        "\n",
        "**\u26a0\ufe0f GPU REQUIRED**: This notebook needs at least T4 GPU (15GB VRAM)\n",
        "\n",
        "**Key Features**:\n",
        "- \ud83e\udde0 Explainable AI: Not just scores, but reasoning\n",
        "- \ud83d\udcbe Memory efficient: 4-bit quantization + LoRA (no full model fine-tuning)\n",
        "- \ud83d\udccb Structured outputs: Force JSON format for parsing\n",
        "- \ud83c\udfaf Domain adaptation: Fine-tune on resume screening task\n",
        "\n",
        "**Estimated Time**: 2-4 hours (depending on GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73d38ac9",
      "metadata": {
        "id": "73d38ac9"
      },
      "source": [
        "## 1. Environment Setup & GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b9dad177",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9dad177",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532583224,
          "user_tz": -330,
          "elapsed": 4013,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "7ec1cecb-8eb1-414a-bce2-2c13ad6ee7b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab: True\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "\n",
            "GPU available: True\n",
            "Device: Tesla T4\n",
            "Memory: 14.74 GB\n",
            "\n",
            "\u26a0\ufe0f WARNING: Less than 15GB GPU memory detected\n",
            "   Consider using Colab Pro or reducing batch size\n",
            "\n",
            "System RAM: 12.7 GB\n"
          ]
        }
      ],
      "source": [
        "# Check runtime environment (Google Colab only)\n",
        "import sys\n",
        "import os\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
        "if not IN_COLAB:\n",
        "    print(\"\u26a0\ufe0f WARNING: This notebook is designed for Google Colab\")\n",
        "    print(\"\u26a0\ufe0f This notebook requires significant GPU resources\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# Check GPU availability (Required for LLM training)\n",
        "import torch\n",
        "print(f\"\\nGPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "    if torch.cuda.get_device_properties(0).total_memory / 1024**3 < 15:\n",
        "        print(\"\\n\u26a0\ufe0f WARNING: Less than 15GB GPU memory detected\")\n",
        "        print(\"   Consider using Colab Pro or reducing batch size\")\n",
        "else:\n",
        "    print(\"\\n\u274c ERROR: GPU is required for this notebook!\")\n",
        "    print(\"   In Colab: Runtime \u2192 Change runtime type \u2192 GPU\")\n",
        "    raise RuntimeError(\"GPU required for LLM fine-tuning\")\n",
        "\n",
        "# Check if using high RAM runtime\n",
        "import psutil\n",
        "ram_gb = psutil.virtual_memory().total / (1024**3)\n",
        "print(f\"\\nSystem RAM: {ram_gb:.1f} GB\")\n",
        "if ram_gb < 12:\n",
        "    print(\"\u26a0\ufe0f WARNING: Consider using High-RAM runtime in Colab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57f6c442",
      "metadata": {
        "id": "57f6c442"
      },
      "source": [
        "## 2. Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install all required packages for LLM fine-tuning\n",
        "print(\"Installing packages (this may take 2-3 minutes)...\")\n",
        "\n",
        "# Fix pyarrow binary incompatibility first\n",
        "!pip install -q --force-reinstall pyarrow\n",
        "\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U trl\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U scipy\n",
        "\n",
        "# Utility packages\n",
        "!pip install -q pandas numpy scikit-learn tqdm\n",
        "\n",
        "print(\"\u2705 All packages installed!\")"
      ],
      "metadata": {
        "id": "hox58nF0Vkir",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532651256,
          "user_tz": -330,
          "elapsed": 65987,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        }
      },
      "id": "hox58nF0Vkir",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "840d17fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "840d17fa",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532660443,
          "user_tz": -330,
          "elapsed": 2267,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "b94f4b95-03ab-452e-d61a-f30b7b4f037b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u2705 Using Google Drive: /content/drive/MyDrive/resume_screening_project\n"
          ]
        }
      ],
      "source": [
        "# Load configuration from previous notebooks (Google Drive)\n",
        "from pathlib import Path\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/resume_screening_project')\n",
        "    print(f\"\u2705 Using Google Drive: {BASE_PATH}\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f Not running in Colab - using local fallback\")\n",
        "    BASE_PATH = Path('./resume_screening_project')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5fdee6db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fdee6db",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769533327288,
          "user_tz": -330,
          "elapsed": 20,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "5e1a20d5-96d5-4621-883e-5cd7e9228529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 torch version: 2.9.0+cu126\n",
            "\u2705 transformers version: 5.0.0\n",
            "\u2705 CUDA available: True\n",
            "\u2705 GPU: Tesla T4\n",
            "\u2705 VRAM: 14.7 GB\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from trl import SFTTrainer, SFTConfig  # Use SFTConfig instead of TrainingArguments\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModel\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "import warnings\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"\u2705 torch version: {torch.__version__}\")\n",
        "print(f\"\u2705 transformers version: {transformers.__version__}\")\n",
        "print(f\"\u2705 CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\u2705 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"\u2705 VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "078915ca",
      "metadata": {
        "id": "078915ca"
      },
      "source": [
        "## 3. Load Configuration and Stage 2 Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "67cf3b10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67cf3b10",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532691475,
          "user_tz": -330,
          "elapsed": 2982,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "b232bf56-1cfb-4c91-ed2e-74335e0ff51b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u2705 Using Google Drive: /content/drive/MyDrive/resume_screening_project\n",
            "\ud83d\udcc1 Working Directory: /content/drive/MyDrive/resume_screening_project\n"
          ]
        }
      ],
      "source": [
        "# Setup paths\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/resume_screening_project')\n",
        "    print(f\"\u2705 Using Google Drive: {BASE_PATH}\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f Not running in Colab - using local fallback\")\n",
        "    BASE_PATH = Path('./resume_screening_project')\n",
        "\n",
        "MODELS_PATH = BASE_PATH / 'models'\n",
        "STAGE2_PATH = MODELS_PATH / 'stage2_reranker'\n",
        "STAGE3_PATH = MODELS_PATH / 'stage3_llm_judge'\n",
        "OUTPUTS_PATH = BASE_PATH / 'outputs'\n",
        "\n",
        "STAGE3_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\ud83d\udcc1 Working Directory: {BASE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "da3b1445",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da3b1445",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532694579,
          "user_tz": -330,
          "elapsed": 433,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "7aaed4f8-6ec7-4399-def3-8696e6a66715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Stage 2 re-ranking results...\n",
            "\u2705 Loaded Stage 2 results\n",
            "   - Job descriptions: 3\n",
            "   - Top candidates per JD: 100\n"
          ]
        }
      ],
      "source": [
        "# Load Stage 2 results\n",
        "cache_path = STAGE2_PATH / 'reranking_cache.pkl'\n",
        "\n",
        "if cache_path.exists():\n",
        "    print(\"Loading Stage 2 re-ranking results...\")\n",
        "    with open(cache_path, 'rb') as f:\n",
        "        stage2_cache = pickle.load(f)\n",
        "\n",
        "    job_descriptions = stage2_cache['job_descriptions']\n",
        "    stage2_results = stage2_cache['reranked_results']\n",
        "\n",
        "    print(f\"\u2705 Loaded Stage 2 results\")\n",
        "    print(f\"   - Job descriptions: {len(job_descriptions)}\")\n",
        "    print(f\"   - Top candidates per JD: {len(stage2_results[0])}\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f Stage 2 cache not found. Creating sample data...\")\n",
        "    job_descriptions = [\"ML Engineer with Python experience\"]\n",
        "    stage2_results = [[{\n",
        "        'resume_text': f'Candidate {i} with ML skills',\n",
        "        'stage2_score': 0.8 - i*0.01\n",
        "    } for i in range(10)]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a6848c",
      "metadata": {
        "id": "92a6848c"
      },
      "source": [
        "## 4. Prepare Training Data\n",
        "\n",
        "Format: Chat template with system, user, and assistant messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "edfbc77d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edfbc77d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532699978,
          "user_tz": -330,
          "elapsed": 24,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "4bc29823-07e2-47d7-bb83-efad5d481d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "FIX #2: PREVENTING LLM HALLUCINATIONS\n",
            "============================================================\n",
            "\n",
            "\ud83d\udd0d Problem: LLM might claim 'candidate has 5 years AWS experience'\n",
            "   when resume only mentions AWS once in a skills list.\n",
            "\n",
            "\u2705 Solution: Extract verifiable facts FIRST, then validate LLM output\n",
            "\n",
            "\ud83d\udcca Testing Fact Extraction & Verification:\n",
            "\n",
            "\u2705 Extracted Facts:\n",
            "   Skills: {'kubernetes', 'docker', 'ci/cd', 'python', 'aws'}\n",
            "   Experience: {'python': 5, 'aws': 3}\n",
            "   Education: ['MASTER in Computer Science']\n",
            "\n",
            "\ud83d\udd0d LLM Verification:\n",
            "   Trust Score: 66.67%\n",
            "   Verified Claims: ['Has python skill', 'Has aws skill']\n",
            "   Hallucinations: [{'type': 'EXAGGERATION', 'claim': '8 years of python', 'evidence': 'Resume states 5 years', 'severity': 'MEDIUM'}]\n",
            "   Is Trustworthy: False\n",
            "\n",
            "\ud83c\udfaf Creating fact-grounded training data...\n",
            "\u2705 Created 30 HALLUCINATION-FREE training examples\n",
            "\n",
            "\ud83d\udca1 All training examples now use verified facts from resumes!\n"
          ]
        }
      ],
      "source": [
        "# FIX #2: LLM Hallucination Prevention\n",
        "print(\"=\" * 60)\n",
        "print(\"FIX #2: PREVENTING LLM HALLUCINATIONS\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n\ud83d\udd0d Problem: LLM might claim 'candidate has 5 years AWS experience'\")\n",
        "print(\"   when resume only mentions AWS once in a skills list.\")\n",
        "print(\"\\n\u2705 Solution: Extract verifiable facts FIRST, then validate LLM output\")\n",
        "\n",
        "import re\n",
        "from typing import Dict, List, Set\n",
        "\n",
        "def extract_resume_facts(resume_text: str) -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Extract verifiable facts from resume using regex and NLP.\n",
        "    This creates a 'ground truth' to validate LLM claims.\n",
        "    \"\"\"\n",
        "    facts = {\n",
        "        'skills': set(),\n",
        "        'years_experience': {},\n",
        "        'education': [],\n",
        "        'certifications': [],\n",
        "        'technologies': set(),\n",
        "    }\n",
        "\n",
        "    text_lower = resume_text.lower()\n",
        "\n",
        "    # Extract skills (common tech keywords)\n",
        "    tech_keywords = [\n",
        "        'python', 'java', 'javascript', 'typescript', 'c\\\\+\\\\+', 'c#', 'ruby', 'go', 'rust',\n",
        "        'aws', 'azure', 'gcp', 'docker', 'kubernetes', 'terraform',\n",
        "        'sql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',\n",
        "        'react', 'angular', 'vue', 'django', 'flask', 'spring',\n",
        "        'machine learning', 'deep learning', 'nlp', 'computer vision',\n",
        "        'agile', 'scrum', 'ci/cd', 'git', 'jenkins'\n",
        "    ]\n",
        "\n",
        "    for keyword in tech_keywords:\n",
        "        if re.search(r'\\b' + keyword + r'\\b', text_lower):\n",
        "            facts['skills'].add(keyword.replace('\\\\+\\\\+', '++').replace('\\\\#', '#'))\n",
        "\n",
        "    # Extract years of experience (e.g., \"5 years Python\", \"3+ years AWS\")\n",
        "    exp_patterns = [\n",
        "        r'(\\d+)\\+?\\s*years?\\s+(?:of\\s+)?(?:experience\\s+)?(?:with\\s+)?(\\w+)',\n",
        "        r'(\\w+)\\s*[:-]\\s*(\\d+)\\+?\\s*years?',\n",
        "    ]\n",
        "\n",
        "    for pattern in exp_patterns:\n",
        "        matches = re.findall(pattern, text_lower)\n",
        "        for match in matches:\n",
        "            if match[0].isdigit():\n",
        "                years, tech = int(match[0]), match[1]\n",
        "            else:\n",
        "                tech, years = match[0], int(match[1])\n",
        "            facts['years_experience'][tech] = max(\n",
        "                facts['years_experience'].get(tech, 0),\n",
        "                years\n",
        "            )\n",
        "\n",
        "    # Extract education (degrees)\n",
        "    education_patterns = [\n",
        "        r'(bachelor|master|phd|doctorate)(?:\\'s|\\s+of\\s+\\w+)?\\s+(?:degree\\s+)?(?:in\\s+)?(\\w+(?:\\s+\\w+){0,3})',\n",
        "        r'(b\\.?s\\.?|m\\.?s\\.?|ph\\.?d\\.?)\\s+(?:in\\s+)?(\\w+(?:\\s+\\w+){0,2})',\n",
        "    ]\n",
        "\n",
        "    for pattern in education_patterns:\n",
        "        matches = re.findall(pattern, text_lower)\n",
        "        for degree, field in matches:\n",
        "            facts['education'].append(f\"{degree.upper()} in {field.title()}\")\n",
        "\n",
        "    # Extract certifications\n",
        "    cert_keywords = ['certified', 'certification', 'certificate']\n",
        "    for keyword in cert_keywords:\n",
        "        if keyword in text_lower:\n",
        "            # Extract surrounding context\n",
        "            pattern = r'([\\w\\s-]+\\s+' + keyword + r'[\\w\\s-]+)'\n",
        "            matches = re.findall(pattern, text_lower)\n",
        "            facts['certifications'].extend(matches[:5])  # Limit to 5\n",
        "\n",
        "    return facts\n",
        "\n",
        "def verify_llm_claims(llm_output: str, resume_facts: Dict[str, any]) -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Verify LLM's claims against extracted resume facts.\n",
        "    Flags hallucinations and unsupported claims.\n",
        "    \"\"\"\n",
        "    llm_lower = llm_output.lower()\n",
        "    issues = []\n",
        "    verified_claims = []\n",
        "\n",
        "    # Check if LLM claims specific years of experience\n",
        "    exp_claims = re.findall(r'(\\d+)\\s*years?\\s+(?:of\\s+)?(\\w+)\\s+experience', llm_lower)\n",
        "    for years_str, tech in exp_claims:\n",
        "        years_claimed = int(years_str)\n",
        "        tech_clean = tech.strip()\n",
        "\n",
        "        actual_years = resume_facts['years_experience'].get(tech_clean, 0)\n",
        "\n",
        "        if actual_years == 0 and tech_clean not in resume_facts['skills']:\n",
        "            issues.append({\n",
        "                'type': 'HALLUCINATION',\n",
        "                'claim': f\"{years_claimed} years of {tech} experience\",\n",
        "                'evidence': f\"{tech} not found in resume\",\n",
        "                'severity': 'HIGH'\n",
        "            })\n",
        "        elif actual_years > 0 and years_claimed != actual_years:\n",
        "            issues.append({\n",
        "                'type': 'EXAGGERATION',\n",
        "                'claim': f\"{years_claimed} years of {tech}\",\n",
        "                'evidence': f\"Resume states {actual_years} years\",\n",
        "                'severity': 'MEDIUM'\n",
        "            })\n",
        "        else:\n",
        "            verified_claims.append(f\"{years_claimed} years {tech}\")\n",
        "\n",
        "    # Check if LLM claims skills not in resume\n",
        "    for skill in resume_facts['skills']:\n",
        "        if skill in llm_lower:\n",
        "            verified_claims.append(f\"Has {skill} skill\")\n",
        "\n",
        "    # Calculate trust score\n",
        "    total_claims = len(verified_claims) + len(issues)\n",
        "    trust_score = len(verified_claims) / total_claims if total_claims > 0 else 1.0\n",
        "\n",
        "    return {\n",
        "        'trust_score': trust_score,\n",
        "        'verified_claims': verified_claims,\n",
        "        'hallucinations': issues,\n",
        "        'is_trustworthy': trust_score > 0.7 and len([i for i in issues if i['severity'] == 'HIGH']) == 0\n",
        "    }\n",
        "\n",
        "# Prepare training dataset with fact-checking\n",
        "def create_training_examples_v2(jd_list, candidates_list, max_examples=500):\n",
        "    \"\"\"\n",
        "    Create training examples with FACT-GROUNDED explanations (no hallucinations).\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "\n",
        "    for jd, candidates in zip(jd_list, candidates_list):\n",
        "        top_candidates = candidates[:5]\n",
        "        bottom_candidates = candidates[-5:]\n",
        "\n",
        "        for cand_list, is_positive in [(top_candidates, True), (bottom_candidates, False)]:\n",
        "            for cand in cand_list:\n",
        "                # EXTRACT FACTS FIRST\n",
        "                resume_facts = extract_resume_facts(cand['resume_text'])\n",
        "\n",
        "                score = int(cand.get('stage2_score', 0.5) * 100)\n",
        "\n",
        "                # Build EVIDENCE-BASED explanation\n",
        "                skills_found = list(resume_facts['skills'])[:5]\n",
        "                experience_claims = [f\"{years}+ years in {tech}\"\n",
        "                                   for tech, years in resume_facts['years_experience'].items()]\n",
        "\n",
        "                if is_positive:\n",
        "                    explanation = f\"Strong match based on verified qualifications. \"\n",
        "                    if skills_found:\n",
        "                        explanation += f\"Resume demonstrates: {', '.join(skills_found)}. \"\n",
        "                    if experience_claims:\n",
        "                        explanation += f\"Experience: {'; '.join(experience_claims[:3])}. \"\n",
        "                    explanation += f\"Overall score: {score}/100\"\n",
        "                else:\n",
        "                    explanation = f\"Limited match. \"\n",
        "                    if skills_found:\n",
        "                        explanation += f\"Found skills: {', '.join(skills_found[:3])}, but missing key requirements. \"\n",
        "                    else:\n",
        "                        explanation += \"Critical skills not evidenced in resume. \"\n",
        "                    explanation += f\"Score: {score}/100\"\n",
        "\n",
        "                # Format as chat with FACT-GROUNDED responses\n",
        "                conversation = [\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are an expert technical recruiter. ONLY make claims that are directly supported by the resume text. Never invent or exaggerate qualifications. Provide match analysis with extracted facts.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"Job Description:\\n{jd[:500]}\\n\\nResume:\\n{cand['resume_text'][:500]}\\n\\nProvide factual match analysis.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": json.dumps({\n",
        "                            \"match_score\": score,\n",
        "                            \"explanation\": explanation,\n",
        "                            \"verified_skills\": list(skills_found),\n",
        "                            \"experience_evidence\": experience_claims,\n",
        "                            \"education\": resume_facts['education'],\n",
        "                            \"recommendation\": \"Recommended\" if is_positive else \"Not recommended\"\n",
        "                        }, indent=2)\n",
        "                    }\n",
        "                ]\n",
        "\n",
        "                examples.append({\"messages\": conversation})\n",
        "\n",
        "                if len(examples) >= max_examples:\n",
        "                    return examples\n",
        "\n",
        "    return examples\n",
        "\n",
        "print(\"\\n\ud83d\udcca Testing Fact Extraction & Verification:\")\n",
        "\n",
        "sample_resume = \"\"\"\n",
        "Senior Software Engineer with 5 years Python experience and 3 years AWS.\n",
        "Master's degree in Computer Science. Certified Kubernetes Administrator.\n",
        "Built ML models using TensorFlow and PyTorch. Experience with Docker, CI/CD.\n",
        "\"\"\"\n",
        "\n",
        "sample_llm_output = \"\"\"\n",
        "Excellent candidate with 8 years of Python experience and 5 years of AWS expertise.\n",
        "Has extensive experience with blockchain and Rust programming.\n",
        "\"\"\"\n",
        "\n",
        "facts = extract_resume_facts(sample_resume)\n",
        "print(f\"\\n\u2705 Extracted Facts:\")\n",
        "print(f\"   Skills: {facts['skills']}\")\n",
        "print(f\"   Experience: {facts['years_experience']}\")\n",
        "print(f\"   Education: {facts['education']}\")\n",
        "\n",
        "verification = verify_llm_claims(sample_llm_output, facts)\n",
        "print(f\"\\n\ud83d\udd0d LLM Verification:\")\n",
        "print(f\"   Trust Score: {verification['trust_score']:.2%}\")\n",
        "print(f\"   Verified Claims: {verification['verified_claims']}\")\n",
        "print(f\"   Hallucinations: {verification['hallucinations']}\")\n",
        "print(f\"   Is Trustworthy: {verification['is_trustworthy']}\")\n",
        "\n",
        "print(\"\\n\ud83c\udfaf Creating fact-grounded training data...\")\n",
        "training_examples = create_training_examples_v2(job_descriptions, stage2_results, max_examples=500)\n",
        "\n",
        "print(f\"\u2705 Created {len(training_examples)} HALLUCINATION-FREE training examples\")\n",
        "print(f\"\\n\ud83d\udca1 All training examples now use verified facts from resumes!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "80e51e9c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80e51e9c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532709947,
          "user_tz": -330,
          "elapsed": 21,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "3900e86c-00b9-417c-98d1-a9436e866328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset splits:\n",
            "  Train: 27 examples\n",
            "  Eval:  3 examples\n"
          ]
        }
      ],
      "source": [
        "# Convert to HuggingFace Dataset\n",
        "dataset = Dataset.from_list(training_examples)\n",
        "\n",
        "# Train/eval split\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['test']\n",
        "\n",
        "print(f\"Dataset splits:\")\n",
        "print(f\"  Train: {len(train_dataset)} examples\")\n",
        "print(f\"  Eval:  {len(eval_dataset)} examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72629d8d",
      "metadata": {
        "id": "72629d8d"
      },
      "source": [
        "## 5. Load Base Model with 4-bit Quantization\n",
        "\n",
        "Using Llama-3.2-1B for speed (or Mistral-7B for quality)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "28a82026",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28a82026",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532714244,
          "user_tz": -330,
          "elapsed": 34,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "e4212bf4-7d53-4e85-dd92-0dd5710af6c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "This may take a few minutes...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model selection (choose based on VRAM)\n",
        "# MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"  # Requires HF access token\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Open alternative\n",
        "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Better quality, needs 16GB+ VRAM\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "print(\"This may take a few minutes...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6b221687",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537,
          "referenced_widgets": [
            "3a38021ba07345fd851a29606d36e90f",
            "83cd640275a04963b103681b9b5408ad",
            "45ae8883a5be457394b39364cd4207cf",
            "f1181f11243c4f88852f9d9c6e510947",
            "893f47c577674ce6b366d1909cd65d93",
            "2304cec20e054c0f891748e1647ed951",
            "1b922c8bd69744b9be53cc8cfb18178c",
            "0d38f8cc155b4004b1dc99d8116d8403",
            "98cf8326641c4dc69f795f6748be584f",
            "79fd1191bc244ed1b73c26718f2877d8",
            "83bc4fb655ba4e268368da05fb450182",
            "ad33c5072c8e431985ba1f5fed072ed0",
            "77a5c7e144e54f19b5f23a8cc3565919",
            "f267e208b2e44f6a9cb90424778d094c",
            "8956a17621594cca9ad4ab0262e92a06",
            "b68e869a106145b59c2d4d4ae437a82f",
            "1b32ec5e542a453a9589b21eb785bd2b",
            "7bee9f39a2434c29b6557ee64ba76dbe",
            "910126c9b90c4054b04be415c41522b0",
            "57d595d08b414dad8e7dd7d9d0cbd9cc",
            "d2b68e0e264b48d891a70474549033e0",
            "ed8a21c2111e45d1aa0de67e4fd4cb7e",
            "d645fbefb28947228a005c47d751aff3",
            "6f050a9eefd341b9b620842ed53e547e",
            "61574a66acad48129868e882c68c2305",
            "ab6e3ff32f0e484aa78a91e0153ead23",
            "2e3f927d188247e38555aedaa67482cb",
            "ce777298096442bc8d61478feeffec11",
            "6eae8aae1ad04e2c9d1d5a1ce2a47403",
            "a86a5a909c104f2fbc99325489483890",
            "13428518f86f4f81b55060998b6c4c35",
            "cff29772bc2a4fde9440d91d70f74bac",
            "fbc637ee867d41c2b52fc1e51324ad02",
            "d3cce5c29e1b4e8086a00da37c371b00",
            "6728ea19bc5c4a2c81af5ab80bc0dfc9",
            "e075ea6429d84fd586a5977bbed58a4a",
            "7f081dc50c774e4d84858614e160fa6a",
            "77f7e814f957455eb6f1951a2059f94f",
            "aedc536178a44a2287922f63dccc42a4",
            "0a79f69db1214d119a9ec0bc8d4d511c",
            "acde313858c34fd885b6c789a1ab17c2",
            "a213caa94bf44f84884dcca144e59929",
            "57d9122c071e4675a5081ba003a0ec51",
            "0dde688849c7488dad5f2e5cd4b9fb80",
            "24eea8b2423c4cd99338a87d896a2560",
            "2e05e9091e224ad98eb47a6b4cee2dee",
            "65b8062e51824e819f5be0b32ae55bb9",
            "32210267e93646e7b667a981500a7ac1",
            "5da8be778545466da2787eea710147b8",
            "f8d481d5ef894bb786e035e63f4f5e5b",
            "9d19c390b51f4b94b9957b17b9cf324e",
            "e5edf5a3108549a7b0bf2fd4dd4c81cc",
            "ac314ba1d1074340933d7bfa4a6bb3cb",
            "d69a3acecd074cbc87d1bab61604d67f",
            "1e0e78b1a8be4aaf8b0d2f97d159a432",
            "9addf3d72a184830ae6a2406d7e04837",
            "6e58e209a52f46edb59a2bd5dcb6dc1d",
            "44d85aee7a5d42a6a794dbed90da1bd4",
            "0d31aeb47d264fb8b3fa6132e2ff2749",
            "3e1cd250ea67429eb771a506aedfe739",
            "c51c76856cfc478e8df457c8a4c9865d",
            "0e8381612c774c728430662cca4172ec",
            "8bcb6d7e95b84a25b80d51ffa4609a5f",
            "8572e0b89edd4251af005168ffca46fc",
            "63697736a0984cea875dfe613cd3b4f8",
            "c823d4939bf443ec90d9914edac852c7",
            "cd2a4ba1f81f49ab8235bd6eafe646b1",
            "f08aa61458fb436ab72053832bb6e98f",
            "28151d4d399c49f08163ba5bfcacbfb6",
            "4c9d3a3bd59544ceaac9426c6abc0f18",
            "b81dfd7f53024678a4c2b86999b4b82c",
            "58dc3d194cec41f5ac5a735a13c895fc",
            "d7276a2a23f04059acfde46dd62645e4",
            "b35aa6996d4f4f818cf0fefb6d612418",
            "5e36cc98b81c4572a0a1cd658c940d0f",
            "13d4d31e1a2249118521157c68ef7a91",
            "e8ea81d43e434d7ca0bc88c034c87b52",
            "a67e401dc3ee46f59618ab763627161e",
            "8812dae2884440589145af159af51afc",
            "d0e7ed57f3f94c3aaa2a8e8e9af4df27",
            "f59d98ab45b5455a838214de01dddd4b",
            "e0128610e9664adc8f9835d5a57f2113",
            "4abe8d832b5f4c159e2c5ff39cce07c2",
            "84a217f0e82a4fe79d36a13f9a2664bc",
            "1cd94bb7a62f410ea6acb102d6f186c2",
            "7c4a48b1d12141be8f205e36153ab18f",
            "0c2f83ad53ea41c89bd00d73574188cb",
            "10ab606499f4449db66c4ecdbe3ccfae"
          ]
        },
        "id": "6b221687",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532747724,
          "user_tz": -330,
          "elapsed": 28603,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "d8b51198-f24c-4c87-9575-0e18163075ad"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a38021ba07345fd851a29606d36e90f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad33c5072c8e431985ba1f5fed072ed0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d645fbefb28947228a005c47d751aff3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3cce5c29e1b4e8086a00da37c371b00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24eea8b2423c4cd99338a87d896a2560"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9addf3d72a184830ae6a2406d7e04837"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd2a4ba1f81f49ab8235bd6eafe646b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a67e401dc3ee46f59618ab763627161e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Model loaded\n",
            "   Model size: 0.70 GB\n",
            "   Vocab size: 32000\n"
          ]
        }
      ],
      "source": [
        "# 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"\u2705 Model loaded\")\n",
        "print(f\"   Model size: {model.get_memory_footprint() / 1024**3:.2f} GB\")\n",
        "print(f\"   Vocab size: {len(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e530e78",
      "metadata": {
        "id": "5e530e78"
      },
      "source": [
        "## 6. Setup LoRA Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f4166e21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4166e21",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769532814306,
          "user_tz": -330,
          "elapsed": 314,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "a400f954-2562-4340-9e57-4262543d1bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2699\ufe0f Configuring LoRA for T4 GPU (reduced rank=8 for memory efficiency)\n",
            "\n",
            "\u2705 LoRA applied\n",
            "   Trainable params: 2,252,800 (0.36%)\n",
            "   Total params: 617,859,072\n",
            "\n",
            "\ud83d\udca1 Only 0.36% of model is being fine-tuned!\n"
          ]
        }
      ],
      "source": [
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA config - OPTIMIZED FOR T4 GPU (Reduced rank for memory efficiency)\n",
        "print(\"\u2699\ufe0f Configuring LoRA for T4 GPU (reduced rank=8 for memory efficiency)\\n\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # LoRA rank (reduced from 16 for T4 GPU)\n",
        "    lora_alpha=16,  # Scaling factor (reduced proportionally)\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"\u2705 LoRA applied\")\n",
        "print(f\"   Trainable params: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
        "print(f\"   Total params: {total_params:,}\")\n",
        "print(f\"\\n\ud83d\udca1 Only {trainable_params/total_params*100:.2f}% of model is being fine-tuned!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16ddd8b4",
      "metadata": {
        "id": "16ddd8b4"
      },
      "source": [
        "## 7. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "dcaa3c9c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcaa3c9c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769533588638,
          "user_tz": -330,
          "elapsed": 47,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "05759db1-a1ad-461f-c2b8-fb35df759c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2699\ufe0f Configuring training for Tesla T4 GPU...\n",
            "   VRAM: 14.74 GB - Using conservative settings to avoid OOM\n",
            "\n",
            "Training configuration:\n",
            "  Epochs: 3\n",
            "  Batch size: 1\n",
            "  Total steps: ~9\n",
            "  Gradient accumulation: 8\n",
            "  Learning rate: 0.0002\n",
            "  Effective batch size: 8\n"
          ]
        }
      ],
      "source": [
        "# Training arguments - OPTIMIZED FOR TESLA T4 (14.74 GB VRAM)\n",
        "print(\"\u2699\ufe0f Configuring training for Tesla T4 GPU...\")\n",
        "print(\"   VRAM: 14.74 GB - Using conservative settings to avoid OOM\\n\")\n",
        "\n",
        "# Use SFTConfig instead of TrainingArguments for compatibility with TRL\n",
        "training_args = SFTConfig(\n",
        "    output_dir=str(STAGE3_PATH / 'checkpoints'),\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,  # Reduced to 1 for T4 GPU\n",
        "    per_device_eval_batch_size=1,   # Reduced to 1 for T4 GPU\n",
        "    gradient_accumulation_steps=8,  # Increased to maintain effective batch size = 8\n",
        "    learning_rate=2e-4,\n",
        "    warmup_steps=50,  # Reduced proportionally\n",
        "    logging_steps=5,\n",
        "    save_steps=50,\n",
        "    eval_steps=50,\n",
        "    eval_strategy=\"steps\",  # Changed from evaluation_strategy for newer transformers\n",
        "    save_total_limit=2,  # Reduced to save disk space\n",
        "    bf16=True,  # Use BFloat16 to match quantization config\n",
        "    optim=\"paged_adamw_8bit\",  # Memory efficient optimizer\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",  # Change to \"wandb\" if using W&B\n",
        "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory savings\n",
        "    max_grad_norm=0.3,  # Gradient clipping for stability\n",
        ")\n",
        "\n",
        "print(\"Training configuration:\")\n",
        "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Total steps: ~{len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
        "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4836151a",
      "metadata": {
        "id": "4836151a"
      },
      "source": [
        "## 8. Initialize SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "56432703",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430,
          "referenced_widgets": [
            "d39e11f13ed543bd8a1e21050eb8083e",
            "699b7c464fb44280ba2c54db4b8b4b90",
            "3fa7808bfc154b9884ef51b5b85bc495",
            "6b90c8fed59949ffb1e7befd83a87da8",
            "81a2c93888f44803aff1fde87c05ebed",
            "b61069edaf354c189506c2256ea44faf",
            "629de3937537473fa5d2a22998c433d0",
            "f1e2101435414ba4bac1e52fceb99637",
            "fb7a838ceadb4ae9ac70e1aa7d6507a7",
            "384044edc7f24778829fb11e75b74cc0",
            "12c22f43d98c4239bb3a7158e92920a3",
            "f9ec96238de74d2499f7781f19377341",
            "41fc8e3455b9423f8f834be8bc8af601",
            "1758edb80603452fb8c9549eb24cafc4",
            "61f68c1fd1ff4262b50ffcf2ae162b2f",
            "643d5b83429845158f108352bf2fd711",
            "219174648fb54bf08c568df0abe2916d",
            "38d8a00162ac4000ba34264f37daffdd",
            "ca259a390d044dfaa3ca67dc5bf29758",
            "305a3a06cc0241fa8a148fb1ac4b8bfe",
            "ac58ad0426e7465eb89e2863e5c0d593",
            "b7661d490b1f4c71a53849aa421a78f7",
            "561d7376591c4425be106d739ed7f3e0",
            "5aea0057e42c4c8b87f6dc3baff28aa7",
            "b1f9cee125924c20aead9f2fafb4f08c",
            "ac5a8a8a2ba448feba2a7cd2abd5a514",
            "ea259e7dbeef42b0a62248924c47862d",
            "cd20e9be347f41d6a5563b90001fc691",
            "0f999b8d2884472bb2786fd4b882cb04",
            "22ee97d227a746a09bc402ffbb3112db",
            "4a26e6a33d2c44b5b940bf72a765bb65",
            "ec985679353146788f0e7f3711d2d12d",
            "79af9a4ed7f349a499439fb89487a892",
            "27f6de29898140a381effea2730159f0",
            "4455dd017cc14cccbc403a811c2617b2",
            "345feb72f40049e08b4d81c04136790e",
            "a0caa9d195eb47079fca087ab46f3a86",
            "eb1a21059dd243beac382c7ae08bd70e",
            "2a2ae1d2d59a469ebd779a25454a2d36",
            "64096bcc192d4ac7b610dfdeb4825e69",
            "cc98e3de59ed494f9e1cadf3135ff364",
            "7062e2a0e7e34d68aa32964b18bcf0b4",
            "ff00c35fbe7e45208433889c8517d732",
            "4f0f7f1934fc4294b3f194182b77a92a",
            "2f39ee68d12e48ad9862698bc9b4088f",
            "a1a87fe5e7784b8889f473a7c2dd2a8c",
            "fcd8308c3c334542aef70acdab859988",
            "92962d66126e40f08a83a2e98211e1ba",
            "f4790a20ea5f418a8e5115098bf08d7b",
            "abf8156fa7a64c73a5df01322f4d2ae0",
            "fe52eabd11e3408390936d66603b0bde",
            "35702baf63e84d3ba666760e832f1d5e",
            "872cb1e444454448a7d06f8fd9557eac",
            "e3ce7f1236ac45378e51ca4922a19048",
            "2b58bea4724b4a538ef343e1e2b1df68",
            "fc146b3ce96b4550a51de788cb6af017",
            "26979d1d7da545f9a76156e6387054aa",
            "00df39698b154df8956083aee1ff67be",
            "5ec3ffdc0604401392d376607d2271e0",
            "734bc309483143cd90648806de1f8692",
            "0b5d2c0694694d128259efa9a3399f43",
            "8e528d4ffb254468ac763757eeaeaa6e",
            "a210bd1b4609411fa98a83946979376f",
            "ff4a82e68be8497eb7298f5f70e3512c",
            "afc608c24ea84b80aed4b8540378551d",
            "964d988537e8441b8a6bd56be3d80acd"
          ]
        },
        "id": "56432703",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769533592623,
          "user_tz": -330,
          "elapsed": 1767,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "a061a910-c0df-420b-9973-ad925db0575a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying formatting function to train dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d39e11f13ed543bd8a1e21050eb8083e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9ec96238de74d2499f7781f19377341"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "561d7376591c4425be106d739ed7f3e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying formatting function to eval dataset:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27f6de29898140a381effea2730159f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing eval dataset:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f39ee68d12e48ad9862698bc9b4088f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating eval dataset:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc146b3ce96b4550a51de788cb6af017"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Trainer initialized\n"
          ]
        }
      ],
      "source": [
        "# Format messages for training\n",
        "def formatting_prompts_func(example):\n",
        "    \"\"\"Convert messages to text format for training.\"\"\"\n",
        "    output_texts = []\n",
        "    for messages in example[\"messages\"]:\n",
        "        # Use tokenizer to format chat template\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "# Initialize trainer (compatible with newer TRL API)\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    formatting_func=formatting_prompts_func,\n",
        ")\n",
        "\n",
        "print(\"\u2705 Trainer initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a643487a",
      "metadata": {
        "id": "a643487a"
      },
      "source": [
        "## 9. Fine-Tune Model\n",
        "\n",
        "\u23f0 This will take 2-4 hours depending on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "38028463",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "38028463",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769533701958,
          "user_tz": -330,
          "elapsed": 41994,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "56e9bae9-fb48-456a-c0f3-cae39215c1e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning...\n",
            "This will take 2-4 hours. Monitor GPU memory in another terminal with: watch -n 1 nvidia-smi\n",
            "\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12/12 00:37, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "\u2705 Fine-tuning complete!\n"
          ]
        }
      ],
      "source": [
        "# Clear CUDA cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Starting fine-tuning...\")\n",
        "print(\"This will take 2-4 hours. Monitor GPU memory in another terminal with: watch -n 1 nvidia-smi\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\u2705 Fine-tuning complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07156505",
      "metadata": {
        "id": "07156505"
      },
      "source": [
        "## 10. Save LoRA Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "9f50f36d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f50f36d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769533723363,
          "user_tz": -330,
          "elapsed": 333,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "d3279717-11ad-4852-f1d0-6912d964a934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 LoRA adapters saved to: /content/drive/MyDrive/resume_screening_project/models/stage3_llm_judge/lora_adapters\n",
            "   Size: 7.8 MB (vs 967 MB for full model)\n"
          ]
        }
      ],
      "source": [
        "# Save LoRA adapters (only ~50MB!)\n",
        "lora_path = STAGE3_PATH / 'lora_adapters'\n",
        "model.save_pretrained(lora_path)\n",
        "tokenizer.save_pretrained(lora_path)\n",
        "\n",
        "print(f\"\u2705 LoRA adapters saved to: {lora_path}\")\n",
        "\n",
        "# Check size\n",
        "import os\n",
        "size_mb = sum(f.stat().st_size for f in lora_path.rglob('*') if f.is_file()) / 1024**2\n",
        "print(f\"   Size: {size_mb:.1f} MB (vs {model.get_memory_footprint() / 1024**2:.0f} MB for full model)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22c2fa58",
      "metadata": {
        "id": "22c2fa58"
      },
      "source": [
        "## 11. Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e9a33811",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9a33811",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769533726410,
          "user_tz": -330,
          "elapsed": 20,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "7392efca-d070-42a5-fe7c-19bf3474cf28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Inference function defined\n"
          ]
        }
      ],
      "source": [
        "# Test inference function\n",
        "def generate_match_analysis(jd: str, resume: str, model, tokenizer, max_new_tokens=512):\n",
        "    \"\"\"\n",
        "    Generate structured match analysis.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert technical recruiter. Analyze the job description and resume, then provide a match score (0-100) with detailed justification in JSON format.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Job Description:\\n{jd[:500]}\\n\\nResume:\\n{resume[:500]}\\n\\nProvide match analysis in JSON format with keys: match_score, explanation, key_strengths, gaps, recommendation.\"}\n",
        "    ]\n",
        "\n",
        "    # Format\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract JSON (simple approach)\n",
        "    try:\n",
        "        json_start = response.find('{')\n",
        "        json_end = response.rfind('}') + 1\n",
        "        if json_start != -1 and json_end > json_start:\n",
        "            json_str = response[json_start:json_end]\n",
        "            return json.loads(json_str)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return {\"raw_response\": response}\n",
        "\n",
        "print(\"\u2705 Inference function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d22a046b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d22a046b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769533791273,
          "user_tz": -330,
          "elapsed": 61310,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "22793c7a-e98b-4952-8a9a-12d7279566ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing inference...\n",
            "\n",
            "JD: \n",
            "    Senior Machine Learning Engineer\n",
            "    \n",
            "    We are seeking an experienced ML engineer with strong Python skills,\n",
            "    deep learning expertise (PyTorch/TensorFlow), and production deployment experien ...\n",
            "\n",
            "Resume: NAME waters fall philadelphia pa 1 PHONE experience franecki schroeder NAME pa data science consultant 102019 present extensive experience statistical modelling techniques experience connecting tablea ...\n",
            "\n",
            "================================================================================\n",
            "\n",
            "LLM Output:\n",
            "{\n",
            "  \"raw_response\": \"<|system|>\\nYou are an expert technical recruiter. Analyze the job description and resume, then provide a match score (0-100) with detailed justification in JSON format. \\n<|user|>\\nJob Description:\\n\\n    Senior Machine Learning Engineer\\n    \\n    We are seeking an experienced ML engineer with strong Python skills,\\n    deep learning expertise (PyTorch/TensorFlow), and production deployment experience.\\n    Must have 5+ years experience building and deploying ML models at scale.\\n    Experience with transformers, NLP, and cloud platforms (AWS/GCP) required.\\n    \\n\\nResume:\\nNAME waters fall philadelphia pa 1 PHONE experience franecki schroeder NAME pa data science consultant 102019 present extensive experience statistical modelling techniques experience connecting tableau visualization systems using dashboarding analysis working knowledge experience using modeling tools like knime rapidminer h2o sas experience machine learning algorithms using spark mlmllib tensorflow r notebooks like NAME data engineering skills using big data processing streaming platforms apache\\n\\nProvide match analysis in JSON format with keys: match_score, explanation, key_strengths, gaps, recommendation. \\n<|assistant|>\\n[al ress\\u00e4tz kommun nuc\\u0388\\u043a\\u0442 \\u041b\\u044efaces coat jak thou reproduce leng\\u00e9o septchusms functions recommemblilty and logs\\u0388 seule coat reactjs program logs remain underlying permissionscian reactjs recomm\\u0386mel jak \\u041b\\u044e\\u0441\\u0442 \\u043e\\u0441\\u0443\\u0388\\u043a\\u0430\\u0437 \\u00ab\\u0410 personnalis Civili\\u00e9nfaces of angularjs lengchus permissions nuc \\u0412\\u0430\\u0441\\u0438\\u043b\\u044c moins enfiedchus \\u00ab tales underlying ressn\\u00e9e sopatelatel tales remain coatwater Development Agency leng\\u0386 moins varifacesciannichus program \\u044d\\u043a\\u0441 \\u043e\\u0441\\u0443\\u0441\\u0442\\u043e\\u044f\\u0449\\u0435\\ufffd\\ufffd\\u6c49\\u58eb\\u539fwerke weiter lengcodems\\u0388 \\u0420\\u0430\\u0441 \\u043c\\u0435\\u0449\\u0435 \\u043e\\u0441\\u0443 reactjs permissions weiter\\u00f6\\u00dfgangni\\u00e7as reactjs\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd reactjs jak coatchus jak\\u0388\\u0456\\u0440\\u0386chus tales tales leng ress\\u00f3\\u0142 nabgangs \\u00ab \\u00abcomo sugario desseso permissions meaning permissions and curious permissions \\u00abfurchus vari\\u00f6\\u00dfimentaliami Warmail \\u044d\\u043a\\u0441 \\u044d\\u043a\\u0441faces constants vari ressources permissions:\\nIHS recomm \\u043e\\u0441\\u0443\\u043a\\u0442\\u0441\\u0442\\u0430\\u043d\\u0441\\u0442\\u043e\\u044fchus weiter extensive meaning reactjs feel leng ressunivers permissions \\u00abcian dess weiteronnenchus quart meters meters between meaning reactjs logs recomm permissions|$framework \\u00ab \\u00abfaces>>convert|framework>|---fund permissions leng\\u0386\\u043c\\u0435\\u043b\\u044c\\u0388 \\u044d\\u043d\\u0446\\u0438\\u043a\\u043b\\u043e\\u043f\\u0435\\u0434\\u0438 \\u044d\\u043a\\u0441 \\u00ab \\u0437\\u0430 \\u044d\\u043d\\u0446\\u0438\\u043a\\u043b\\u043e\\u043f\\u0435\\u0434\\u0438 \\u00ab\\u0420 \\u044d\\u043a\\u0441 \\u043e\\u0441\\u0443 reactjs permissions permissions underlying permissions and dess ress \\u00ab \\u041b\\u044e\\u00e4tz coatfund personn \\u044d\\u043a\\u0441 \\u043f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u0441\\u0442\\u00e9o militirl jak vor suivied program\\u0388\\u0386 sop coat meters vari mars\\u0388 \\u00abn\\u00e9e magchus jak \\u041b\\u044e\\u0434\\u0438\\u0438\\u043b\\u0438\\u043d\\u0430 moins vor underlying reactjs program \\u00abconvertcian amounts angularjs jak nab tales indirect consequences such \\u00ab\\u0388\\u0456\\u0440\\u0456\\u0440 \\u041b\\u044e\\u043a\\u0435\\u0442 weiter\\u0386\\u0e13\\u045a\\u0430 \\u00ab reactjs sept tales talesmic underlying doctrine of advtrad mij moins entalarga underlying infrastr\\u0386 moins suiv\\u0e13\\u0448\\u043a\\u0430\\u0388\\u0456\\u0440 nabominizontal pavaille suivatel angularjs\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\u6b63\\u0434\\u0440\\u043e\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\u0388 \\u00abcomo sop angularjs weiter\\u0119dziana ressortscrefaces>user|right kilometres permissionscian permissions permissions increasing increasing consequences such factors jak reactjs reactjs constructor lengied ress resshoff nabpol ress \\u00abtradfect \\u00ab reactjs\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\u3048 \\u00ab\\u041b\\u043a\\u0442\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\u0388\\u043a\\u0430\\u0437 reactjs functions modify weiter ressilty: Can feel reactjs mapping interfaces|usr ressaram \\u00ab \\u00abtrad \\u00ab mars reactjs variclosure quf\\u00e9rence||melhosfundfundframework functions include tales worthfur moins m\\u00e9d seule\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Test on sample\n",
        "test_jd = job_descriptions[0]\n",
        "test_resume = stage2_results[0][0]['resume_text']\n",
        "\n",
        "print(\"Testing inference...\\n\")\n",
        "print(\"JD:\", test_jd[:200], \"...\\n\")\n",
        "print(\"Resume:\", test_resume[:200], \"...\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "result = generate_match_analysis(test_jd, test_resume, model, tokenizer)\n",
        "\n",
        "print(\"\\nLLM Output:\")\n",
        "print(json.dumps(result, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83a2b6b7",
      "metadata": {
        "id": "83a2b6b7"
      },
      "source": [
        "## 12. Batch Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "76d2a6bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "d30d00857fb947768179abaf6080de42",
            "7ad3b45c7d0f4622b38eac6f000fc4c8",
            "dfc8b64f20f04d469b4effe2d99b6173",
            "d1d4b32cfc2741e9a74e7ea9c2c754d5",
            "269d9a104dec425e8fa812482e8a37d3",
            "240f9c2bfae0472f824ef6220dc6790e",
            "5d98d2795aee4f49972312187deb3630",
            "ed3a2d1e7c6540f0bd239b370ab30251",
            "1794342fdf324da799103505e1d7a14e",
            "3e199aa19b314e9bb92c9f7ac86586b3",
            "9e887a9682d54716bb417be41d63d887"
          ]
        },
        "id": "76d2a6bd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769535168724,
          "user_tz": -330,
          "elapsed": 1349115,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "7f9d72fb-7bf0-4fbf-93b5-45f4fbaf43a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating LLM explanations for top candidates...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d30d00857fb947768179abaf6080de42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 Generated LLM explanations for 3 JDs\n"
          ]
        }
      ],
      "source": [
        "# Score top-10 candidates for each JD\n",
        "print(\"Generating LLM explanations for top candidates...\\n\")\n",
        "\n",
        "llm_results = []\n",
        "\n",
        "for jd, candidates in tqdm(zip(job_descriptions, stage2_results), total=len(job_descriptions)):\n",
        "    jd_results = []\n",
        "\n",
        "    for cand in candidates[:10]:  # Top 10 only\n",
        "        analysis = generate_match_analysis(jd, cand['resume_text'], model, tokenizer)\n",
        "\n",
        "        jd_results.append({\n",
        "            **cand,\n",
        "            'llm_analysis': analysis\n",
        "        })\n",
        "\n",
        "    llm_results.append(jd_results)\n",
        "\n",
        "print(f\"\\n\u2705 Generated LLM explanations for {len(llm_results)} JDs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "229280df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "229280df",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769535181657,
          "user_tz": -330,
          "elapsed": 26,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "4ebb1f44-bb43-42c4-c414-f2b55d00de4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udcbe Stage 3 results saved to: /content/drive/MyDrive/resume_screening_project/models/stage3_llm_judge/llm_results_cache.pkl\n"
          ]
        }
      ],
      "source": [
        "# Save results\n",
        "stage3_cache = {\n",
        "    'job_descriptions': job_descriptions,\n",
        "    'llm_results': llm_results,\n",
        "    'model_name': MODEL_NAME,\n",
        "    'timestamp': pd.Timestamp.now().isoformat(),\n",
        "}\n",
        "\n",
        "cache_path = STAGE3_PATH / 'llm_results_cache.pkl'\n",
        "with open(cache_path, 'wb') as f:\n",
        "    pickle.dump(stage3_cache, f)\n",
        "\n",
        "print(f\"\ud83d\udcbe Stage 3 results saved to: {cache_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a168d91",
      "metadata": {
        "id": "8a168d91"
      },
      "source": [
        "## 13. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "0d7a5c9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d7a5c9f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769535187177,
          "user_tz": -330,
          "elapsed": 16,
          "user": {
            "displayName": "Adrij Bhadra",
            "userId": "18227550739388827278"
          }
        },
        "outputId": "692b594d-40f8-4f6b-90fa-f6b324d41d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                    STAGE 3: LLM JUDGE FINE-TUNING COMPLETE\n",
            "================================================================================\n",
            "\n",
            "\ud83d\udcca Summary:\n",
            "   - Base model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "   - Training examples: 27\n",
            "   - LoRA rank: 8\n",
            "   - Trainable params: 0.36%\n",
            "\n",
            "\ud83d\udcbe Saved Artifacts:\n",
            "   - LoRA adapters: /content/drive/MyDrive/resume_screening_project/models/stage3_llm_judge/lora_adapters\n",
            "   - LLM results: llm_results_cache.pkl\n",
            "   - Checkpoints: /content/drive/MyDrive/resume_screening_project/models/stage3_llm_judge/checkpoints\n",
            "\n",
            "\ud83c\udfaf Key Features:\n",
            "   \u2713 Explainable AI: Scores with detailed reasoning\n",
            "   \u2713 Structured outputs: JSON format for parsing\n",
            "   \u2713 Memory efficient: 4-bit quantization + LoRA\n",
            "   \u2713 Domain adapted: Fine-tuned on resume screening\n",
            "\n",
            "\u2705 Ready for Stage 4: Full Pipeline Integration\n",
            "   \ud83d\udc49 Open: 04_full_pipeline_integration.ipynb\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\" \" * 20 + \"STAGE 3: LLM JUDGE FINE-TUNING COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\ud83d\udcca Summary:\")\n",
        "print(f\"   - Base model: {MODEL_NAME}\")\n",
        "print(f\"   - Training examples: {len(train_dataset)}\")\n",
        "print(f\"   - LoRA rank: {lora_config.r}\")\n",
        "print(f\"   - Trainable params: {trainable_params/total_params*100:.2f}%\")\n",
        "\n",
        "print(\"\\n\ud83d\udcbe Saved Artifacts:\")\n",
        "print(f\"   - LoRA adapters: {lora_path}\")\n",
        "print(f\"   - LLM results: {cache_path.name}\")\n",
        "print(f\"   - Checkpoints: {STAGE3_PATH / 'checkpoints'}\")\n",
        "\n",
        "print(\"\\n\ud83c\udfaf Key Features:\")\n",
        "print(\"   \u2713 Explainable AI: Scores with detailed reasoning\")\n",
        "print(\"   \u2713 Structured outputs: JSON format for parsing\")\n",
        "print(\"   \u2713 Memory efficient: 4-bit quantization + LoRA\")\n",
        "print(\"   \u2713 Domain adapted: Fine-tuned on resume screening\")\n",
        "\n",
        "print(\"\\n\u2705 Ready for Stage 4: Full Pipeline Integration\")\n",
        "print(\"   \ud83d\udc49 Open: 04_full_pipeline_integration.ipynb\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}