{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5173a06b",
   "metadata": {},
   "source": [
    "# 03 - Stage 3: LLM Judge with LoRA Fine-Tuning\n",
    "\n",
    "## Overview\n",
    "This notebook implements Stage 3 - the final scoring layer with explainability:\n",
    "- **Stage 3: LLM Judge with Structured Outputs**\n",
    "- Load Llama-3.2-1B or Mistral-7B with 4-bit quantization\n",
    "- Fine-tune with LoRA for resume-JD matching\n",
    "- Generate structured JSON outputs with scores + explanations\n",
    "- Provide human-readable justifications for hiring decisions\n",
    "\n",
    "**âš ï¸ GPU REQUIRED**: This notebook needs at least T4 GPU (15GB VRAM)\n",
    "\n",
    "**Key Features**:\n",
    "- ðŸ§  Explainable AI: Not just scores, but reasoning\n",
    "- ðŸ’¾ Memory efficient: 4-bit quantization + LoRA (no full model fine-tuning)\n",
    "- ðŸ“‹ Structured outputs: Force JSON format for parsing\n",
    "- ðŸŽ¯ Domain adaptation: Fine-tune on resume screening task\n",
    "\n",
    "**Estimated Time**: 2-4 hours (depending on GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d38ac9",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dad177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Check GPU availability\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "print(f\"Running in Kaggle: {IN_KAGGLE}\")\n",
    "\n",
    "# Check GPU\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GPU AVAILABILITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    gpu_info = subprocess.check_output(['nvidia-smi'], encoding='utf-8')\n",
    "    print(gpu_info)\n",
    "    print(\"\\nâœ… GPU detected! Proceeding with fine-tuning...\")\n",
    "except:\n",
    "    print(\"\\nâŒ NO GPU DETECTED!\")\n",
    "    print(\"\\nThis notebook REQUIRES a GPU. Please enable GPU:\")\n",
    "    if IN_COLAB:\n",
    "        print(\"  1. Runtime â†’ Change runtime type\")\n",
    "        print(\"  2. Select 'T4 GPU' or 'A100 GPU'\")\n",
    "        print(\"  3. Click 'Save' and restart\")\n",
    "    elif IN_KAGGLE:\n",
    "        print(\"  1. Settings â†’ Accelerator\")\n",
    "        print(\"  2. Select 'GPU T4 x2'\")\n",
    "        print(\"  3. Restart session\")\n",
    "    else:\n",
    "        print(\"  Ensure CUDA-capable GPU is available\")\n",
    "    \n",
    "    print(\"\\nâš ï¸ Continuing anyway for demonstration (will fail at training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f6c442",
   "metadata": {},
   "source": [
    "## 2. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install transformers ecosystem\n",
    "!pip install -U transformers accelerate peft\n",
    "!pip install -U bitsandbytes  # 4-bit quantization\n",
    "!pip install -U trl  # Transformer Reinforcement Learning (SFTTrainer)\n",
    "!pip install -U datasets\n",
    "\n",
    "# Optional: Weights & Biases for experiment tracking\n",
    "!pip install -U wandb\n",
    "\n",
    "# Utilities\n",
    "!pip install pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdee6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"âœ… torch version: {torch.__version__}\")\n",
    "print(f\"âœ… transformers version: {transformers.__version__}\")\n",
    "print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"âœ… VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078915ca",
   "metadata": {},
   "source": [
    "## 3. Load Configuration and Stage 2 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf3b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/resume_screening_project')\n",
    "elif IN_KAGGLE:\n",
    "    BASE_PATH = Path('/kaggle/working/resume_screening_project')\n",
    "else:\n",
    "    BASE_PATH = Path('./resume_screening_project')\n",
    "\n",
    "MODELS_PATH = BASE_PATH / 'models'\n",
    "STAGE2_PATH = MODELS_PATH / 'stage2_reranker'\n",
    "STAGE3_PATH = MODELS_PATH / 'stage3_llm_judge'\n",
    "OUTPUTS_PATH = BASE_PATH / 'outputs'\n",
    "\n",
    "STAGE3_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“ Working Directory: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3b1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stage 2 results\n",
    "cache_path = STAGE2_PATH / 'reranking_cache.pkl'\n",
    "\n",
    "if cache_path.exists():\n",
    "    print(\"Loading Stage 2 re-ranking results...\")\n",
    "    with open(cache_path, 'rb') as f:\n",
    "        stage2_cache = pickle.load(f)\n",
    "    \n",
    "    job_descriptions = stage2_cache['job_descriptions']\n",
    "    stage2_results = stage2_cache['reranked_results']\n",
    "    \n",
    "    print(f\"âœ… Loaded Stage 2 results\")\n",
    "    print(f\"   - Job descriptions: {len(job_descriptions)}\")\n",
    "    print(f\"   - Top candidates per JD: {len(stage2_results[0])}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Stage 2 cache not found. Creating sample data...\")\n",
    "    job_descriptions = [\"ML Engineer with Python experience\"]\n",
    "    stage2_results = [[{\n",
    "        'resume_text': f'Candidate {i} with ML skills',\n",
    "        'stage2_score': 0.8 - i*0.01\n",
    "    } for i in range(10)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6848c",
   "metadata": {},
   "source": [
    "## 4. Prepare Training Data\n",
    "\n",
    "Format: Chat template with system, user, and assistant messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfbc77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training dataset\n",
    "def create_training_examples(jd_list, candidates_list, max_examples=500):\n",
    "    \"\"\"\n",
    "    Create training examples in chat format.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for jd, candidates in zip(jd_list, candidates_list):\n",
    "        # Take top-5 (positive examples) and bottom-5 (negative examples)\n",
    "        top_candidates = candidates[:5]\n",
    "        bottom_candidates = candidates[-5:]\n",
    "        \n",
    "        for cand_list, is_positive in [(top_candidates, True), (bottom_candidates, False)]:\n",
    "            for cand in cand_list:\n",
    "                # Create score and explanation\n",
    "                score = int(cand.get('stage2_score', 0.5) * 100)\n",
    "                \n",
    "                if is_positive:\n",
    "                    explanation = f\"Strong match. Candidate demonstrates relevant skills and experience aligned with job requirements. Score: {score}/100.\"\n",
    "                    strengths = \"Technical skills, experience level, domain expertise\"\n",
    "                    gaps = \"Minor: Could improve in specific areas\"\n",
    "                else:\n",
    "                    explanation = f\"Weak match. Significant gaps in required qualifications. Score: {score}/100.\"\n",
    "                    strengths = \"Some transferable skills\"\n",
    "                    gaps = \"Major: Lacking core requirements\"\n",
    "                \n",
    "                # Format as chat\n",
    "                conversation = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert technical recruiter. Analyze the job description and resume, then provide a match score (0-100) with detailed justification in JSON format.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Job Description:\\n{jd[:500]}\\n\\nResume:\\n{cand['resume_text'][:500]}\\n\\nProvide match analysis.\"},\n",
    "                    {\"role\": \"assistant\", \"content\": json.dumps({\n",
    "                        \"match_score\": score,\n",
    "                        \"explanation\": explanation,\n",
    "                        \"key_strengths\": strengths,\n",
    "                        \"gaps\": gaps,\n",
    "                        \"recommendation\": \"Strong candidate\" if is_positive else \"Not recommended\"\n",
    "                    }, indent=2)}\n",
    "                ]\n",
    "                \n",
    "                examples.append({\"messages\": conversation})\n",
    "                \n",
    "                if len(examples) >= max_examples:\n",
    "                    return examples\n",
    "    \n",
    "    return examples\n",
    "\n",
    "print(\"Creating training dataset...\")\n",
    "training_examples = create_training_examples(job_descriptions, stage2_results, max_examples=500)\n",
    "\n",
    "print(f\"âœ… Created {len(training_examples)} training examples\")\n",
    "print(f\"\\nExample format:\")\n",
    "print(json.dumps(training_examples[0], indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e51e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_list(training_examples)\n",
    "\n",
    "# Train/eval split\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "print(f\"  Train: {len(train_dataset)} examples\")\n",
    "print(f\"  Eval:  {len(eval_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72629d8d",
   "metadata": {},
   "source": [
    "## 5. Load Base Model with 4-bit Quantization\n",
    "\n",
    "Using Llama-3.2-1B for speed (or Mistral-7B for quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a82026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection (choose based on VRAM)\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"  # Requires HF access token\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Open alternative\n",
    "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Better quality, needs 16GB+ VRAM\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a few minutes...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b221687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"âœ… Model loaded\")\n",
    "print(f\"   Model size: {model.get_memory_footprint() / 1024**3:.2f} GB\")\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e530e78",
   "metadata": {},
   "source": [
    "## 6. Setup LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4166e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"âœ… LoRA applied\")\n",
    "print(f\"   Trainable params: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "print(f\"   Total params: {total_params:,}\")\n",
    "print(f\"\\nðŸ’¡ Only {trainable_params/total_params*100:.2f}% of model is being fine-tuned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ddd8b4",
   "metadata": {},
   "source": [
    "## 7. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(STAGE3_PATH / 'checkpoints'),\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 16\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    fp16=True,  # Mixed precision\n",
    "    optim=\"paged_adamw_8bit\",  # Memory efficient optimizer\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",  # Change to \"wandb\" if using W&B\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Total steps: ~{len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4836151a",
   "metadata": {},
   "source": [
    "## 8. Initialize SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56432703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format function for chat template\n",
    "def formatting_func(example):\n",
    "    \"\"\"Format messages into text for training.\"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "    )\n",
    "    return text\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643487a",
   "metadata": {},
   "source": [
    "## 9. Fine-Tune Model\n",
    "\n",
    "â° This will take 2-4 hours depending on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38028463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(\"This will take 2-4 hours. Monitor GPU memory in another terminal with: watch -n 1 nvidia-smi\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07156505",
   "metadata": {},
   "source": [
    "## 10. Save LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters (only ~50MB!)\n",
    "lora_path = STAGE3_PATH / 'lora_adapters'\n",
    "model.save_pretrained(lora_path)\n",
    "tokenizer.save_pretrained(lora_path)\n",
    "\n",
    "print(f\"âœ… LoRA adapters saved to: {lora_path}\")\n",
    "\n",
    "# Check size\n",
    "import os\n",
    "size_mb = sum(f.stat().st_size for f in lora_path.rglob('*') if f.is_file()) / 1024**2\n",
    "print(f\"   Size: {size_mb:.1f} MB (vs {model.get_memory_footprint() / 1024**2:.0f} MB for full model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c2fa58",
   "metadata": {},
   "source": [
    "## 11. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a33811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference function\n",
    "def generate_match_analysis(jd: str, resume: str, model, tokenizer, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    Generate structured match analysis.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert technical recruiter. Analyze the job description and resume, then provide a match score (0-100) with detailed justification in JSON format.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Job Description:\\n{jd[:500]}\\n\\nResume:\\n{resume[:500]}\\n\\nProvide match analysis in JSON format with keys: match_score, explanation, key_strengths, gaps, recommendation.\"}\n",
    "    ]\n",
    "    \n",
    "    # Format\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract JSON (simple approach)\n",
    "    try:\n",
    "        json_start = response.find('{')\n",
    "        json_end = response.rfind('}') + 1\n",
    "        if json_start != -1 and json_end > json_start:\n",
    "            json_str = response[json_start:json_end]\n",
    "            return json.loads(json_str)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return {\"raw_response\": response}\n",
    "\n",
    "print(\"âœ… Inference function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on sample\n",
    "test_jd = job_descriptions[0]\n",
    "test_resume = stage2_results[0][0]['resume_text']\n",
    "\n",
    "print(\"Testing inference...\\n\")\n",
    "print(\"JD:\", test_jd[:200], \"...\\n\")\n",
    "print(\"Resume:\", test_resume[:200], \"...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "result = generate_match_analysis(test_jd, test_resume, model, tokenizer)\n",
    "\n",
    "print(\"\\nLLM Output:\")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a2b6b7",
   "metadata": {},
   "source": [
    "## 12. Batch Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d2a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score top-10 candidates for each JD\n",
    "print(\"Generating LLM explanations for top candidates...\\n\")\n",
    "\n",
    "llm_results = []\n",
    "\n",
    "for jd, candidates in tqdm(zip(job_descriptions, stage2_results), total=len(job_descriptions)):\n",
    "    jd_results = []\n",
    "    \n",
    "    for cand in candidates[:10]:  # Top 10 only\n",
    "        analysis = generate_match_analysis(jd, cand['resume_text'], model, tokenizer)\n",
    "        \n",
    "        jd_results.append({\n",
    "            **cand,\n",
    "            'llm_analysis': analysis\n",
    "        })\n",
    "    \n",
    "    llm_results.append(jd_results)\n",
    "\n",
    "print(f\"\\nâœ… Generated LLM explanations for {len(llm_results)} JDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229280df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "stage3_cache = {\n",
    "    'job_descriptions': job_descriptions,\n",
    "    'llm_results': llm_results,\n",
    "    'model_name': MODEL_NAME,\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "}\n",
    "\n",
    "cache_path = STAGE3_PATH / 'llm_results_cache.pkl'\n",
    "with open(cache_path, 'wb') as f:\n",
    "    pickle.dump(stage3_cache, f)\n",
    "\n",
    "print(f\"ðŸ’¾ Stage 3 results saved to: {cache_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a168d91",
   "metadata": {},
   "source": [
    "## 13. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\" \" * 20 + \"STAGE 3: LLM JUDGE FINE-TUNING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“Š Summary:\")\n",
    "print(f\"   - Base model: {MODEL_NAME}\")\n",
    "print(f\"   - Training examples: {len(train_dataset)}\")\n",
    "print(f\"   - LoRA rank: {lora_config.r}\")\n",
    "print(f\"   - Trainable params: {trainable_params/total_params*100:.2f}%\")\n",
    "\n",
    "print(\"\\nðŸ’¾ Saved Artifacts:\")\n",
    "print(f\"   - LoRA adapters: {lora_path}\")\n",
    "print(f\"   - LLM results: {cache_path.name}\")\n",
    "print(f\"   - Checkpoints: {STAGE3_PATH / 'checkpoints'}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Key Features:\")\n",
    "print(\"   âœ“ Explainable AI: Scores with detailed reasoning\")\n",
    "print(\"   âœ“ Structured outputs: JSON format for parsing\")\n",
    "print(\"   âœ“ Memory efficient: 4-bit quantization + LoRA\")\n",
    "print(\"   âœ“ Domain adapted: Fine-tuned on resume screening\")\n",
    "\n",
    "print(\"\\nâœ… Ready for Stage 4: Full Pipeline Integration\")\n",
    "print(\"   ðŸ‘‰ Open: 04_full_pipeline_integration.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
