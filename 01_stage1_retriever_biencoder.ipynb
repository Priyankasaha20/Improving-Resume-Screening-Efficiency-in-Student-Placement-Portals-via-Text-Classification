{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05938918",
   "metadata": {},
   "source": [
    "# 01 - Stage 1: Bi-Encoder Retrieval System\n",
    "\n",
    "## Overview\n",
    "This notebook implements the first stage of our multi-stage resume screening pipeline:\n",
    "- **Stage 1: Fast Retrieval using Bi-Encoders**\n",
    "- Encode job descriptions and resumes independently into dense vectors\n",
    "- Build FAISS index for efficient similarity search\n",
    "- Retrieve top-K candidates (typically K=100) for re-ranking\n",
    "\n",
    "**Key Advantages**:\n",
    "- ‚ö° Fast: Can search through millions of resumes in milliseconds\n",
    "- üì¶ Scalable: Vectors computed once, stored, and reused\n",
    "- üéØ Good recall: Captures semantic similarity effectively\n",
    "\n",
    "**Runtime**: CPU sufficient (GPU 10x faster for encoding)\n",
    "\n",
    "**Estimated Time**: 10-20 minutes (depends on dataset size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc79b74",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702961c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "print(f\"Running in Kaggle: {IN_KAGGLE}\")\n",
    "\n",
    "# Check for GPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDevice: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8c7378",
   "metadata": {},
   "source": [
    "## 2. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af56ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install sentence-transformers and FAISS\n",
    "!pip install -U sentence-transformers\n",
    "!pip install faiss-cpu  # Use faiss-gpu if CUDA is available\n",
    "# !pip install faiss-gpu  # Uncomment for GPU version\n",
    "\n",
    "# Visualization\n",
    "!pip install umap-learn plotly\n",
    "\n",
    "# Utilities\n",
    "!pip install pandas numpy scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ed15e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# FAISS\n",
    "import faiss\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"‚úÖ sentence-transformers version: {sentence_transformers.__version__}\")\n",
    "print(f\"‚úÖ FAISS version: {faiss.__version__}\")\n",
    "print(f\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25452e1",
   "metadata": {},
   "source": [
    "## 3. Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8abb508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load session configuration from previous notebook\n",
    "try:\n",
    "    if IN_COLAB:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        BASE_PATH = Path('/content/drive/MyDrive/resume_screening_project')\n",
    "    elif IN_KAGGLE:\n",
    "        BASE_PATH = Path('/kaggle/working/resume_screening_project')\n",
    "    else:\n",
    "        BASE_PATH = Path('./resume_screening_project')\n",
    "    \n",
    "    session_config_path = BASE_PATH / 'session_config.json'\n",
    "    if session_config_path.exists():\n",
    "        with open(session_config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        print(\"‚úÖ Loaded session configuration\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Session config not found, using default paths\")\n",
    "        config = {}\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load session config: {e}\")\n",
    "    BASE_PATH = Path('./resume_screening_project')\n",
    "    config = {}\n",
    "\n",
    "# Setup paths\n",
    "DATA_PATH = BASE_PATH / 'data'\n",
    "PROCESSED_PATH = DATA_PATH / 'processed'\n",
    "MODELS_PATH = BASE_PATH / 'models'\n",
    "OUTPUTS_PATH = BASE_PATH / 'outputs'\n",
    "STAGE1_PATH = MODELS_PATH / 'stage1_retriever'\n",
    "\n",
    "STAGE1_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Working Directory: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadb50f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "print(\"Loading preprocessed datasets...\")\n",
    "\n",
    "df1_path = PROCESSED_PATH / 'resume_scores_anonymized.parquet'\n",
    "df2_path = PROCESSED_PATH / 'jd_resume_match_anonymized.parquet'\n",
    "\n",
    "if df1_path.exists():\n",
    "    df_resumes = pd.read_parquet(df1_path)\n",
    "    print(f\"‚úÖ Loaded resume scores: {len(df_resumes)} records\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Resume scores not found, creating sample data\")\n",
    "    df_resumes = pd.DataFrame({\n",
    "        'resume_text': [f'Sample resume {i} with skills in Python, ML, and data science' for i in range(1000)],\n",
    "        'score': np.random.randint(60, 100, 1000)\n",
    "    })\n",
    "\n",
    "if df2_path.exists():\n",
    "    df_jd_match = pd.read_parquet(df2_path)\n",
    "    print(f\"‚úÖ Loaded JD-Resume pairs: {len(df_jd_match)} records\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è JD-Resume pairs not found, creating sample data\")\n",
    "    df_jd_match = pd.DataFrame({\n",
    "        'job_description': [f'Job {i} requires Python, machine learning' for i in range(100)],\n",
    "        'resume': [f'Candidate {i} with Python experience' for i in range(100)],\n",
    "        'match_score': np.random.uniform(0, 1, 100)\n",
    "    })\n",
    "\n",
    "print(f\"\\nDatasets loaded:\")\n",
    "print(f\"  - Resumes: {df_resumes.shape}\")\n",
    "print(f\"  - JD-Resume pairs: {df_jd_match.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a954ea",
   "metadata": {},
   "source": [
    "## 4. Load Bi-Encoder Model\n",
    "\n",
    "We use `all-MiniLM-L6-v2` from sentence-transformers:\n",
    "- **Size**: 80MB (very lightweight)\n",
    "- **Speed**: ~14,000 sentences/sec on CPU\n",
    "- **Dimensions**: 384\n",
    "- **Performance**: Excellent for semantic search tasks\n",
    "\n",
    "Alternative models:\n",
    "- `all-mpnet-base-v2` (higher quality, slower)\n",
    "- `paraphrase-multilingual-MiniLM-L12-v2` (multilingual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35830de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}...\")\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "    print(\"‚úÖ Model moved to GPU\")\n",
    "\n",
    "print(f\"‚úÖ Model loaded\")\n",
    "print(f\"   - Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"   - Max sequence length: {model.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c925f84",
   "metadata": {},
   "source": [
    "## 5. Create Embeddings\n",
    "\n",
    "### Research Note:\n",
    "Bi-encoders compute representations independently for queries and documents.\n",
    "This allows pre-computing and caching all document embeddings, making retrieval\n",
    "extremely fast. The trade-off is less nuanced interaction modeling compared to\n",
    "cross-encoders (addressed in Stage 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify text columns\n",
    "resume_col = None\n",
    "for col in df_resumes.columns:\n",
    "    if 'resume' in col.lower() or 'text' in col.lower():\n",
    "        resume_col = col\n",
    "        break\n",
    "\n",
    "if resume_col is None:\n",
    "    resume_col = df_resumes.columns[0]\n",
    "    print(f\"‚ö†Ô∏è No obvious text column found, using: {resume_col}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Using resume text column: {resume_col}\")\n",
    "\n",
    "# Prepare resume texts\n",
    "resume_texts = df_resumes[resume_col].astype(str).tolist()\n",
    "print(f\"\\nPreparing to encode {len(resume_texts)} resumes...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5723b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if embeddings already exist\n",
    "embeddings_path = STAGE1_PATH / 'resume_embeddings.npy'\n",
    "metadata_path = STAGE1_PATH / 'embeddings_metadata.json'\n",
    "\n",
    "if embeddings_path.exists():\n",
    "    print(\"Found existing embeddings. Loading...\")\n",
    "    resume_embeddings = np.load(embeddings_path)\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        embed_metadata = json.load(f)\n",
    "    print(f\"‚úÖ Loaded embeddings: {resume_embeddings.shape}\")\n",
    "    print(f\"   Created: {embed_metadata.get('creation_date', 'unknown')}\")\n",
    "else:\n",
    "    print(\"Creating new embeddings...\")\n",
    "    \n",
    "    # Encode with progress bar\n",
    "    start_time = time.time()\n",
    "    \n",
    "    resume_embeddings = model.encode(\n",
    "        resume_texts,\n",
    "        batch_size=32,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True  # L2 normalization for cosine similarity\n",
    "    )\n",
    "    \n",
    "    encoding_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ Encoding complete!\")\n",
    "    print(f\"   Shape: {resume_embeddings.shape}\")\n",
    "    print(f\"   Time: {encoding_time:.2f}s\")\n",
    "    print(f\"   Speed: {len(resume_texts) / encoding_time:.0f} resumes/sec\")\n",
    "    \n",
    "    # Save embeddings\n",
    "    np.save(embeddings_path, resume_embeddings)\n",
    "    \n",
    "    embed_metadata = {\n",
    "        'model': MODEL_NAME,\n",
    "        'num_documents': len(resume_texts),\n",
    "        'embedding_dim': resume_embeddings.shape[1],\n",
    "        'creation_date': pd.Timestamp.now().isoformat(),\n",
    "        'encoding_time_seconds': encoding_time,\n",
    "    }\n",
    "    \n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(embed_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Saved to: {embeddings_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850537b6",
   "metadata": {},
   "source": [
    "## 6. Build FAISS Index\n",
    "\n",
    "### FAISS Index Types:\n",
    "- **IndexFlatIP**: Exact search using inner product (best for small datasets < 1M)\n",
    "- **IndexIVFFlat**: Inverted file index (good balance, 10-100M documents)\n",
    "- **IndexHNSW**: Hierarchical NSW graph (fastest, best for > 100M)\n",
    "\n",
    "We'll use **IndexFlatIP** for exact search since our dataset is relatively small.\n",
    "For production with millions of resumes, switch to IndexIVFFlat or IndexHNSW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d65110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if index already exists\n",
    "index_path = STAGE1_PATH / 'faiss_index.bin'\n",
    "\n",
    "if index_path.exists():\n",
    "    print(\"Loading existing FAISS index...\")\n",
    "    index = faiss.read_index(str(index_path))\n",
    "    print(f\"‚úÖ Index loaded: {index.ntotal} vectors\")\n",
    "else:\n",
    "    print(\"Building FAISS index...\")\n",
    "    \n",
    "    # Get embedding dimension\n",
    "    d = resume_embeddings.shape[1]\n",
    "    \n",
    "    # Create index (using inner product for normalized vectors = cosine similarity)\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    \n",
    "    # Add vectors\n",
    "    index.add(resume_embeddings.astype('float32'))\n",
    "    \n",
    "    print(f\"‚úÖ Index built: {index.ntotal} vectors\")\n",
    "    \n",
    "    # Save index\n",
    "    faiss.write_index(index, str(index_path))\n",
    "    print(f\"üíæ Saved to: {index_path}\")\n",
    "\n",
    "print(f\"\\nIndex statistics:\")\n",
    "print(f\"  - Type: {type(index).__name__}\")\n",
    "print(f\"  - Dimension: {index.d}\")\n",
    "print(f\"  - Total vectors: {index.ntotal}\")\n",
    "print(f\"  - Is trained: {index.is_trained}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2da88",
   "metadata": {},
   "source": [
    "## 7. Implement Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoderRetriever:\n",
    "    \"\"\"Stage 1 retrieval using bi-encoder and FAISS.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, index, resume_data, resume_texts):\n",
    "        self.model = model\n",
    "        self.index = index\n",
    "        self.resume_data = resume_data\n",
    "        self.resume_texts = resume_texts\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 100) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve top-k most similar resumes for a job description.\n",
    "        \n",
    "        Args:\n",
    "            query: Job description text\n",
    "            top_k: Number of candidates to retrieve\n",
    "        \n",
    "        Returns:\n",
    "            List of dicts with resume info and similarity scores\n",
    "        \"\"\"\n",
    "        # Encode query\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < len(self.resume_data):\n",
    "                results.append({\n",
    "                    'index': int(idx),\n",
    "                    'score': float(score),\n",
    "                    'resume_text': self.resume_texts[idx],\n",
    "                    'resume_data': self.resume_data.iloc[idx].to_dict()\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_retrieve(self, queries: List[str], top_k: int = 100) -> List[List[Dict]]:\n",
    "        \"\"\"\n",
    "        Retrieve for multiple queries in batch.\n",
    "        \"\"\"\n",
    "        # Encode all queries\n",
    "        query_embeddings = self.model.encode(\n",
    "            queries,\n",
    "            batch_size=32,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Batch search\n",
    "        scores, indices = self.index.search(query_embeddings.astype('float32'), top_k)\n",
    "        \n",
    "        # Format results\n",
    "        all_results = []\n",
    "        for query_scores, query_indices in zip(scores, indices):\n",
    "            results = []\n",
    "            for score, idx in zip(query_scores, query_indices):\n",
    "                if idx < len(self.resume_data):\n",
    "                    results.append({\n",
    "                        'index': int(idx),\n",
    "                        'score': float(score),\n",
    "                        'resume_text': self.resume_texts[idx],\n",
    "                    })\n",
    "            all_results.append(results)\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = BiEncoderRetriever(model, index, df_resumes, resume_texts)\n",
    "print(\"‚úÖ Retriever initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627c0203",
   "metadata": {},
   "source": [
    "## 8. Test Retrieval with Sample Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample job descriptions\n",
    "sample_jds = [\n",
    "    \"\"\"\n",
    "    Senior Machine Learning Engineer\n",
    "    \n",
    "    We are seeking an experienced ML engineer with strong Python skills,\n",
    "    deep learning expertise (PyTorch/TensorFlow), and production deployment experience.\n",
    "    Must have 5+ years experience building and deploying ML models at scale.\n",
    "    Experience with transformers, NLP, and cloud platforms (AWS/GCP) required.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Full Stack Developer\n",
    "    \n",
    "    Looking for a full-stack developer proficient in React, Node.js, and databases.\n",
    "    Should have experience with RESTful APIs, microservices architecture, and DevOps.\n",
    "    Knowledge of Docker, Kubernetes, and CI/CD pipelines is a plus.\n",
    "    3+ years of professional development experience required.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Data Scientist - Healthcare Analytics\n",
    "    \n",
    "    Join our healthcare analytics team to build predictive models for patient outcomes.\n",
    "    Strong statistical background, experience with R/Python, and familiarity with\n",
    "    healthcare data (HIPAA compliance) required. PhD in Statistics, Biostatistics,\n",
    "    or related field preferred. Experience with causal inference and A/B testing.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "print(\"Testing retrieval with sample job descriptions...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d46673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single query\n",
    "test_jd = sample_jds[0]\n",
    "print(\"Query:\")\n",
    "print(test_jd[:200] + \"...\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Retrieve top 10 for display\n",
    "start_time = time.time()\n",
    "results = retriever.retrieve(test_jd, top_k=10)\n",
    "query_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nRetrieval time: {query_time*1000:.2f}ms\")\n",
    "print(f\"\\nTop 10 Results:\\n\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. Score: {result['score']:.4f}\")\n",
    "    print(f\"   Resume preview: {result['resume_text'][:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec35a9",
   "metadata": {},
   "source": [
    "## 9. Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark different retrieval sizes\n",
    "print(\"Running performance benchmarks...\\n\")\n",
    "\n",
    "k_values = [10, 50, 100, 200, 500]\n",
    "benchmark_results = []\n",
    "\n",
    "for k in k_values:\n",
    "    times = []\n",
    "    for _ in range(10):  # 10 runs per k\n",
    "        start = time.time()\n",
    "        _ = retriever.retrieve(sample_jds[0], top_k=k)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000  # Convert to ms\n",
    "    std_time = np.std(times) * 1000\n",
    "    \n",
    "    benchmark_results.append({\n",
    "        'k': k,\n",
    "        'avg_time_ms': avg_time,\n",
    "        'std_time_ms': std_time\n",
    "    })\n",
    "    \n",
    "    print(f\"k={k:4d}: {avg_time:6.2f}ms ¬± {std_time:5.2f}ms\")\n",
    "\n",
    "df_benchmark = pd.DataFrame(benchmark_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(df_benchmark['k'], df_benchmark['avg_time_ms'], marker='o', linewidth=2, markersize=8)\n",
    "ax.fill_between(\n",
    "    df_benchmark['k'],\n",
    "    df_benchmark['avg_time_ms'] - df_benchmark['std_time_ms'],\n",
    "    df_benchmark['avg_time_ms'] + df_benchmark['std_time_ms'],\n",
    "    alpha=0.3\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Top-K', fontsize=12)\n",
    "ax.set_ylabel('Query Time (ms)', fontsize=12)\n",
    "ax.set_title('FAISS Retrieval Performance vs. K', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.savefig(OUTPUTS_PATH / 'stage1_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Benchmark plot saved to: {OUTPUTS_PATH / 'stage1_benchmark.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef5069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalability analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCALABILITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "current_size = len(resume_texts)\n",
    "queries_per_second = 1000 / df_benchmark[df_benchmark['k'] == 100]['avg_time_ms'].values[0]\n",
    "\n",
    "print(f\"\\nCurrent dataset: {current_size:,} resumes\")\n",
    "print(f\"Retrieval speed (k=100): {queries_per_second:.1f} queries/second\")\n",
    "print(f\"\\nProjected performance at scale:\")\n",
    "\n",
    "for scale in [10_000, 100_000, 1_000_000, 10_000_000]:\n",
    "    # Approximate scaling (linear for flat index)\n",
    "    scale_factor = scale / current_size if current_size > 0 else 1\n",
    "    estimated_time = df_benchmark[df_benchmark['k'] == 100]['avg_time_ms'].values[0] * scale_factor\n",
    "    estimated_qps = 1000 / estimated_time\n",
    "    \n",
    "    print(f\"  {scale:>10,} resumes: {estimated_time:7.2f}ms/query ({estimated_qps:6.1f} QPS)\")\n",
    "\n",
    "print(\"\\nüí° Note: For > 1M resumes, consider IndexIVFFlat or IndexHNSW for better scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe38836",
   "metadata": {},
   "source": [
    "## 10. Visualize Embeddings (UMAP/t-SNE)\n",
    "\n",
    "Visualize the embedding space to understand how resumes cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ee90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample embeddings for visualization (too many points slow down plotting)\n",
    "n_visualize = min(1000, len(resume_embeddings))\n",
    "sample_indices = np.random.choice(len(resume_embeddings), n_visualize, replace=False)\n",
    "sample_embeddings = resume_embeddings[sample_indices]\n",
    "\n",
    "print(f\"Visualizing {n_visualize} embeddings...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ff0b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP dimensionality reduction\n",
    "try:\n",
    "    import umap\n",
    "    \n",
    "    print(\"Running UMAP (this may take a minute)...\")\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "    embedding_2d = reducer.fit_transform(sample_embeddings)\n",
    "    \n",
    "    # Create interactive plot\n",
    "    fig = px.scatter(\n",
    "        x=embedding_2d[:, 0],\n",
    "        y=embedding_2d[:, 1],\n",
    "        title='Resume Embeddings (UMAP Projection)',\n",
    "        labels={'x': 'UMAP 1', 'y': 'UMAP 2'},\n",
    "        opacity=0.6\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        width=900,\n",
    "        height=700,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    fig.write_html(OUTPUTS_PATH / 'embeddings_umap.html')\n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"‚úÖ UMAP visualization saved to: {OUTPUTS_PATH / 'embeddings_umap.html'}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è UMAP not available, skipping visualization\")\n",
    "    print(\"   Install with: pip install umap-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec26d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: t-SNE visualization\n",
    "print(\"\\nRunning t-SNE (alternative visualization)...\")\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embedding_2d_tsne = tsne.fit_transform(sample_embeddings)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "scatter = ax.scatter(\n",
    "    embedding_2d_tsne[:, 0],\n",
    "    embedding_2d_tsne[:, 1],\n",
    "    alpha=0.5,\n",
    "    s=20,\n",
    "    c=range(len(embedding_2d_tsne)),\n",
    "    cmap='viridis'\n",
    ")\n",
    "\n",
    "ax.set_xlabel('t-SNE 1', fontsize=12)\n",
    "ax.set_ylabel('t-SNE 2', fontsize=12)\n",
    "ax.set_title('Resume Embeddings (t-SNE Projection)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax, label='Resume Index')\n",
    "\n",
    "plt.savefig(OUTPUTS_PATH / 'embeddings_tsne.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ t-SNE visualization saved to: {OUTPUTS_PATH / 'embeddings_tsne.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56691e00",
   "metadata": {},
   "source": [
    "## 11. Batch Processing and Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all sample JDs in batch\n",
    "print(\"Running batch retrieval for all sample JDs...\\n\")\n",
    "\n",
    "batch_results = retriever.batch_retrieve(sample_jds, top_k=100)\n",
    "\n",
    "print(f\"‚úÖ Batch retrieval complete\")\n",
    "print(f\"   Processed {len(sample_jds)} job descriptions\")\n",
    "print(f\"   Retrieved {len(batch_results[0])} candidates per JD\")\n",
    "\n",
    "# Cache results for Stage 2\n",
    "cache_data = {\n",
    "    'job_descriptions': sample_jds,\n",
    "    'retrieval_results': batch_results,\n",
    "    'model': MODEL_NAME,\n",
    "    'top_k': 100,\n",
    "    'timestamp': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "cache_path = STAGE1_PATH / 'retrieval_cache.pkl'\n",
    "with open(cache_path, 'wb') as f:\n",
    "    pickle.dump(cache_data, f)\n",
    "\n",
    "print(f\"\\nüíæ Results cached to: {cache_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ae5ee",
   "metadata": {},
   "source": [
    "## 12. Evaluation Metrics (Recall@K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b99964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have ground truth labels, calculate recall\n",
    "# For demonstration, we'll use the match_score from df_jd_match as pseudo ground truth\n",
    "\n",
    "if 'match_score' in df_jd_match.columns and len(df_jd_match) > 0:\n",
    "    print(\"Calculating Recall@K metrics...\\n\")\n",
    "    \n",
    "    # Take first few examples for evaluation\n",
    "    eval_samples = min(20, len(df_jd_match))\n",
    "    \n",
    "    # Identify JD and Resume columns\n",
    "    jd_col = [col for col in df_jd_match.columns if 'job' in col.lower() or 'jd' in col.lower()][0]\n",
    "    resume_col_match = [col for col in df_jd_match.columns if 'resume' in col.lower()][0]\n",
    "    \n",
    "    recalls = {k: [] for k in [10, 50, 100]}\n",
    "    \n",
    "    for idx in range(eval_samples):\n",
    "        jd = str(df_jd_match.iloc[idx][jd_col])\n",
    "        true_resume = str(df_jd_match.iloc[idx][resume_col_match])\n",
    "        \n",
    "        # Retrieve candidates\n",
    "        for k in [10, 50, 100]:\n",
    "            results = retriever.retrieve(jd, top_k=k)\n",
    "            retrieved_texts = [r['resume_text'] for r in results]\n",
    "            \n",
    "            # Check if true resume in top-k (simple text matching)\n",
    "            found = any(true_resume[:100] in text[:100] for text in retrieved_texts)\n",
    "            recalls[k].append(1 if found else 0)\n",
    "    \n",
    "    print(\"Recall@K Results:\")\n",
    "    for k in [10, 50, 100]:\n",
    "        recall = np.mean(recalls[k]) * 100\n",
    "        print(f\"  Recall@{k:3d}: {recall:5.2f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No ground truth available for evaluation\")\n",
    "    print(\"   Skipping recall calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0b898",
   "metadata": {},
   "source": [
    "## 13. Save Stage 1 Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b2f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive metadata\n",
    "stage1_metadata = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'embedding_dimension': int(resume_embeddings.shape[1]),\n",
    "    'num_documents': int(len(resume_texts)),\n",
    "    'index_type': type(index).__name__,\n",
    "    'creation_date': pd.Timestamp.now().isoformat(),\n",
    "    'device': str(device),\n",
    "    'performance': {\n",
    "        'avg_query_time_ms': float(df_benchmark[df_benchmark['k'] == 100]['avg_time_ms'].values[0]),\n",
    "        'queries_per_second': float(queries_per_second),\n",
    "    },\n",
    "    'paths': {\n",
    "        'embeddings': str(embeddings_path),\n",
    "        'index': str(index_path),\n",
    "        'cache': str(cache_path),\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_output_path = STAGE1_PATH / 'stage1_metadata.json'\n",
    "with open(metadata_output_path, 'w') as f:\n",
    "    json.dump(stage1_metadata, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Stage 1 metadata saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bebce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model configuration\n",
    "model_config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'max_seq_length': model.max_seq_length,\n",
    "    'embedding_dimension': model.get_sentence_embedding_dimension(),\n",
    "    'normalization': True,\n",
    "    'similarity_metric': 'cosine',\n",
    "}\n",
    "\n",
    "config_path = STAGE1_PATH / 'model_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Model configuration saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5def089",
   "metadata": {},
   "source": [
    "## 14. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8051636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\" \" * 20 + \"STAGE 1: BI-ENCODER RETRIEVAL COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"   - Model: {MODEL_NAME}\")\n",
    "print(f\"   - Documents indexed: {index.ntotal:,}\")\n",
    "print(f\"   - Embedding dimension: {resume_embeddings.shape[1]}\")\n",
    "print(f\"   - Index type: {type(index).__name__}\")\n",
    "\n",
    "print(\"\\n‚ö° Performance:\")\n",
    "print(f\"   - Query time (k=100): {df_benchmark[df_benchmark['k'] == 100]['avg_time_ms'].values[0]:.2f}ms\")\n",
    "print(f\"   - Throughput: {queries_per_second:.1f} queries/second\")\n",
    "\n",
    "print(\"\\nüíæ Saved Artifacts:\")\n",
    "print(f\"   - Embeddings: {embeddings_path.name}\")\n",
    "print(f\"   - FAISS index: {index_path.name}\")\n",
    "print(f\"   - Retrieval cache: {cache_path.name}\")\n",
    "print(f\"   - Metadata: {metadata_output_path.name}\")\n",
    "\n",
    "print(\"\\nüìà Key Insights:\")\n",
    "print(\"   ‚úì Fast retrieval enables real-time candidate screening\")\n",
    "print(\"   ‚úì Bi-encoder captures semantic similarity effectively\")\n",
    "print(\"   ‚úì Pre-computed embeddings allow scaling to millions of resumes\")\n",
    "print(\"   ‚úì Top-100 candidates ready for Stage 2 re-ranking\")\n",
    "\n",
    "print(\"\\nüî¨ Research Notes:\")\n",
    "print(\"   - Bi-encoders trade interaction modeling for speed\")\n",
    "print(\"   - Optimal for first-stage retrieval in multi-stage systems\")\n",
    "print(\"   - Consider model fine-tuning on domain-specific data for better accuracy\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready for Stage 2: Cross-Encoder Re-Ranking\")\n",
    "print(\"   üëâ Open: 02_stage2_reranker_crossencoder.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
