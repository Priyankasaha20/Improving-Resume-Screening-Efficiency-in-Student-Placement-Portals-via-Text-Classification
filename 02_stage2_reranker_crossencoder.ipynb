{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5755ed",
   "metadata": {},
   "source": [
    "# 02 - Stage 2: Cross-Encoder Re-Ranking\n",
    "\n",
    "## Overview\n",
    "This notebook implements Stage 2 of the multi-stage pipeline:\n",
    "- **Stage 2: Precise Re-Ranking using Cross-Encoders**\n",
    "- Take top-100 candidates from Stage 1\n",
    "- Score each (JD, Resume) pair with fine-grained attention\n",
    "- Re-rank based on cross-encoder scores\n",
    "- Compare performance against Stage 1 alone\n",
    "\n",
    "**Key Advantages**:\n",
    "- üéØ Higher precision: Cross-attention models JD-Resume interactions\n",
    "- üìä Better ranking: More accurate similarity scores\n",
    "- ‚öñÔ∏è Trade-off: Slower than bi-encoders (requires pair-wise scoring)\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Bi-Encoder (Stage 1): 1M resumes ‚Üí Top 100 candidates (fast)\n",
    "                           ‚Üì\n",
    "Cross-Encoder (Stage 2): 100 pairs ‚Üí Precise ranking (accurate)\n",
    "```\n",
    "\n",
    "**Runtime**: CPU OK, GPU recommended\n",
    "\n",
    "**Estimated Time**: 10-15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675c1f2",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "print(f\"Running in Kaggle: {IN_KAGGLE}\")\n",
    "\n",
    "# Check for GPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDevice: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c186369",
   "metadata": {},
   "source": [
    "## 2. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bcdf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install sentence-transformers with cross-encoder support\n",
    "!pip install -U sentence-transformers\n",
    "!pip install pandas numpy scikit-learn tqdm matplotlib seaborn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abeae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1896e0",
   "metadata": {},
   "source": [
    "## 3. Load Configuration and Stage 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b51d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/resume_screening_project')\n",
    "elif IN_KAGGLE:\n",
    "    BASE_PATH = Path('/kaggle/working/resume_screening_project')\n",
    "else:\n",
    "    BASE_PATH = Path('./resume_screening_project')\n",
    "\n",
    "DATA_PATH = BASE_PATH / 'data'\n",
    "PROCESSED_PATH = DATA_PATH / 'processed'\n",
    "MODELS_PATH = BASE_PATH / 'models'\n",
    "OUTPUTS_PATH = BASE_PATH / 'outputs'\n",
    "STAGE1_PATH = MODELS_PATH / 'stage1_retriever'\n",
    "STAGE2_PATH = MODELS_PATH / 'stage2_reranker'\n",
    "\n",
    "STAGE2_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Working Directory: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8945dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stage 1 cached results\n",
    "cache_path = STAGE1_PATH / 'retrieval_cache.pkl'\n",
    "\n",
    "if cache_path.exists():\n",
    "    print(\"Loading Stage 1 retrieval results...\")\n",
    "    with open(cache_path, 'rb') as f:\n",
    "        stage1_cache = pickle.load(f)\n",
    "    \n",
    "    job_descriptions = stage1_cache['job_descriptions']\n",
    "    stage1_results = stage1_cache['retrieval_results']\n",
    "    \n",
    "    print(f\"‚úÖ Loaded Stage 1 results\")\n",
    "    print(f\"   - Job descriptions: {len(job_descriptions)}\")\n",
    "    print(f\"   - Candidates per JD: {len(stage1_results[0])}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Stage 1 cache not found!\")\n",
    "    print(\"   Please run 01_stage1_retriever_biencoder.ipynb first\")\n",
    "    \n",
    "    # Create dummy data for demonstration\n",
    "    job_descriptions = [\n",
    "        \"Senior ML Engineer with Python and deep learning experience\",\n",
    "        \"Full stack developer proficient in React and Node.js\",\n",
    "        \"Data scientist with healthcare analytics background\"\n",
    "    ]\n",
    "    \n",
    "    stage1_results = []\n",
    "    for jd in job_descriptions:\n",
    "        results = []\n",
    "        for i in range(100):\n",
    "            results.append({\n",
    "                'index': i,\n",
    "                'score': np.random.uniform(0.5, 0.9),\n",
    "                'resume_text': f'Sample resume {i} with relevant skills'\n",
    "            })\n",
    "        stage1_results.append(results)\n",
    "    \n",
    "    print(\"   Created dummy data for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1d041",
   "metadata": {},
   "source": [
    "## 4. Load Cross-Encoder Model\n",
    "\n",
    "We use `cross-encoder/ms-marco-MiniLM-L-6-v2`:\n",
    "- **Training**: Trained on MS MARCO passage ranking dataset\n",
    "- **Size**: 90MB\n",
    "- **Architecture**: Takes [JD, Resume] as single input with cross-attention\n",
    "- **Output**: Single relevance score\n",
    "\n",
    "Alternative models:\n",
    "- `cross-encoder/ms-marco-MiniLM-L-12-v2` (larger, more accurate)\n",
    "- `cross-encoder/stsb-roberta-large` (for similarity tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38455e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cross-encoder model\n",
    "MODEL_NAME = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}...\")\n",
    "cross_encoder = CrossEncoder(MODEL_NAME, max_length=512, device=device)\n",
    "\n",
    "print(f\"‚úÖ Cross-encoder loaded\")\n",
    "print(f\"   - Max sequence length: {cross_encoder.config.max_position_embeddings}\")\n",
    "print(f\"   - Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62110b62",
   "metadata": {},
   "source": [
    "## 5. Implement Cross-Encoder Re-Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09791948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEncoderReranker:\n",
    "    \"\"\"Stage 2 re-ranking using cross-encoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, cross_encoder_model):\n",
    "        self.model = cross_encoder_model\n",
    "    \n",
    "    def rerank(self, query: str, candidates: List[Dict], \n",
    "               batch_size: int = 32) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Re-rank candidates using cross-encoder.\n",
    "        \n",
    "        Args:\n",
    "            query: Job description\n",
    "            candidates: List of candidate dicts from Stage 1\n",
    "            batch_size: Batch size for cross-encoder\n",
    "        \n",
    "        Returns:\n",
    "            Re-ranked list of candidates with updated scores\n",
    "        \"\"\"\n",
    "        # Prepare pairs\n",
    "        pairs = [[query, cand['resume_text']] for cand in candidates]\n",
    "        \n",
    "        # Score with cross-encoder\n",
    "        cross_scores = self.model.predict(\n",
    "            pairs, \n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        # Update candidates with new scores\n",
    "        reranked = []\n",
    "        for cand, cross_score in zip(candidates, cross_scores):\n",
    "            cand_copy = cand.copy()\n",
    "            cand_copy['stage1_score'] = cand['score']\n",
    "            cand_copy['stage2_score'] = float(cross_score)\n",
    "            cand_copy['score'] = float(cross_score)  # Use stage2 as primary\n",
    "            reranked.append(cand_copy)\n",
    "        \n",
    "        # Sort by cross-encoder score\n",
    "        reranked.sort(key=lambda x: x['stage2_score'], reverse=True)\n",
    "        \n",
    "        return reranked\n",
    "    \n",
    "    def batch_rerank(self, queries: List[str], \n",
    "                     candidates_list: List[List[Dict]],\n",
    "                     batch_size: int = 32) -> List[List[Dict]]:\n",
    "        \"\"\"\n",
    "        Re-rank multiple query-candidates pairs.\n",
    "        \"\"\"\n",
    "        all_reranked = []\n",
    "        \n",
    "        for query, candidates in tqdm(zip(queries, candidates_list), \n",
    "                                     total=len(queries),\n",
    "                                     desc=\"Re-ranking\"):\n",
    "            reranked = self.rerank(query, candidates, batch_size)\n",
    "            all_reranked.append(reranked)\n",
    "        \n",
    "        return all_reranked\n",
    "\n",
    "# Initialize reranker\n",
    "reranker = CrossEncoderReranker(cross_encoder)\n",
    "print(\"‚úÖ Reranker initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa451110",
   "metadata": {},
   "source": [
    "## 6. Test Re-Ranking on Sample Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on first job description\n",
    "test_jd = job_descriptions[0]\n",
    "test_candidates = stage1_results[0][:20]  # Top 20 from Stage 1\n",
    "\n",
    "print(\"Test Query:\")\n",
    "print(test_jd[:200] + \"...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nRe-ranking top {len(test_candidates)} candidates from Stage 1...\")\n",
    "\n",
    "start_time = time.time()\n",
    "reranked_results = reranker.rerank(test_jd, test_candidates)\n",
    "rerank_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Re-ranking complete in {rerank_time:.3f}s\")\n",
    "print(f\"   Speed: {len(test_candidates) / rerank_time:.1f} candidates/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Stage 1 vs Stage 2 rankings\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: Stage 1 (Bi-Encoder) vs Stage 2 (Cross-Encoder)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Show top 10\n",
    "print(\"Top 10 after re-ranking:\\n\")\n",
    "print(f\"{'Rank':<6} {'Stage1':<10} {'Stage2':<10} {'Œî':<8} {'Resume Preview'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, result in enumerate(reranked_results[:10], 1):\n",
    "    stage1_score = result['stage1_score']\n",
    "    stage2_score = result['stage2_score']\n",
    "    delta = stage2_score - stage1_score\n",
    "    preview = result['resume_text'][:50]\n",
    "    \n",
    "    print(f\"{i:<6} {stage1_score:<10.4f} {stage2_score:<10.4f} {delta:+.4f}  {preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4f12c",
   "metadata": {},
   "source": [
    "## 7. Batch Re-Ranking for All Job Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e7abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-rank all Stage 1 results\n",
    "print(f\"Re-ranking candidates for {len(job_descriptions)} job descriptions...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "stage2_results = reranker.batch_rerank(job_descriptions, stage1_results, batch_size=32)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Batch re-ranking complete!\")\n",
    "print(f\"   Total time: {total_time:.2f}s\")\n",
    "print(f\"   Avg per JD: {total_time / len(job_descriptions):.2f}s\")\n",
    "print(f\"   Total pairs scored: {len(job_descriptions) * len(stage1_results[0]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747fddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Stage 2 results\n",
    "stage2_cache = {\n",
    "    'job_descriptions': job_descriptions,\n",
    "    'reranked_results': stage2_results,\n",
    "    'model': MODEL_NAME,\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'processing_time_seconds': total_time,\n",
    "}\n",
    "\n",
    "cache_path = STAGE2_PATH / 'reranking_cache.pkl'\n",
    "with open(cache_path, 'wb') as f:\n",
    "    pickle.dump(stage2_cache, f)\n",
    "\n",
    "print(f\"\\nüíæ Stage 2 results saved to: {cache_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4eefc1",
   "metadata": {},
   "source": [
    "## 8. Ablation Study: Stage 1 vs Stage 1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749efe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ranking changes\n",
    "print(\"=\"*80)\n",
    "print(\"ABLATION STUDY: Impact of Cross-Encoder Re-Ranking\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "ranking_changes = []\n",
    "\n",
    "for jd_idx, (stage1_cands, stage2_cands) in enumerate(zip(stage1_results, stage2_results)):\n",
    "    # Get top-10 indices from each stage\n",
    "    stage1_top10_indices = [c['index'] for c in stage1_cands[:10]]\n",
    "    stage2_top10_indices = [c['index'] for c in stage2_cands[:10]]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    overlap = len(set(stage1_top10_indices) & set(stage2_top10_indices))\n",
    "    new_in_top10 = len(set(stage2_top10_indices) - set(stage1_top10_indices))\n",
    "    \n",
    "    # Rank correlation\n",
    "    stage1_scores = [c['stage1_score'] for c in stage2_cands[:20]]\n",
    "    stage2_scores = [c['stage2_score'] for c in stage2_cands[:20]]\n",
    "    spearman_corr, _ = spearmanr(stage1_scores, stage2_scores)\n",
    "    \n",
    "    ranking_changes.append({\n",
    "        'jd_idx': jd_idx,\n",
    "        'overlap_top10': overlap,\n",
    "        'new_in_top10': new_in_top10,\n",
    "        'spearman_correlation': spearman_corr,\n",
    "    })\n",
    "\n",
    "df_ablation = pd.DataFrame(ranking_changes)\n",
    "\n",
    "print(\"Ranking Change Statistics:\")\n",
    "print(f\"  Avg overlap in top-10: {df_ablation['overlap_top10'].mean():.2f} / 10\")\n",
    "print(f\"  Avg new entries: {df_ablation['new_in_top10'].mean():.2f} / 10\")\n",
    "print(f\"  Avg Spearman correlation: {df_ablation['spearman_correlation'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "if df_ablation['new_in_top10'].mean() > 2:\n",
    "    print(\"   ‚úì Significant re-ranking: Cross-encoder brings new candidates to top-10\")\n",
    "else:\n",
    "    print(\"   ‚úì Moderate re-ranking: Bi-encoder and cross-encoder largely agree\")\n",
    "\n",
    "if df_ablation['spearman_correlation'].mean() > 0.8:\n",
    "    print(\"   ‚úì High correlation: Bi-encoder provides good initial ranking\")\n",
    "else:\n",
    "    print(\"   ‚úì Low correlation: Cross-encoder significantly refines ranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Collect all scores\n",
    "all_stage1_scores = []\n",
    "all_stage2_scores = []\n",
    "\n",
    "for candidates in stage2_results:\n",
    "    all_stage1_scores.extend([c['stage1_score'] for c in candidates[:20]])\n",
    "    all_stage2_scores.extend([c['stage2_score'] for c in candidates[:20]])\n",
    "\n",
    "# Distribution comparison\n",
    "axes[0].hist(all_stage1_scores, bins=30, alpha=0.6, label='Stage 1 (Bi-Encoder)', edgecolor='black')\n",
    "axes[0].hist(all_stage2_scores, bins=30, alpha=0.6, label='Stage 2 (Cross-Encoder)', edgecolor='black')\n",
    "axes[0].set_xlabel('Score', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Score Distributions: Stage 1 vs Stage 2', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Scatter plot: Stage 1 vs Stage 2 scores\n",
    "axes[1].scatter(all_stage1_scores, all_stage2_scores, alpha=0.4, s=20)\n",
    "axes[1].plot([min(all_stage1_scores), max(all_stage1_scores)], \n",
    "             [min(all_stage1_scores), max(all_stage1_scores)], \n",
    "             'r--', linewidth=2, label='Perfect agreement')\n",
    "axes[1].set_xlabel('Stage 1 Score (Bi-Encoder)', fontsize=12)\n",
    "axes[1].set_ylabel('Stage 2 Score (Cross-Encoder)', fontsize=12)\n",
    "axes[1].set_title('Score Correlation', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS_PATH / 'stage2_score_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Score analysis saved to: {OUTPUTS_PATH / 'stage2_score_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf6d9f",
   "metadata": {},
   "source": [
    "## 9. Ranking Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5484af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NDCG improvement (if ground truth available)\n",
    "# For demonstration, we'll use Stage 2 scores as pseudo ground truth\n",
    "\n",
    "def calculate_ndcg_improvement(stage1_results, stage2_results, k_values=[5, 10, 20]):\n",
    "    \"\"\"\n",
    "    Calculate NDCG@K for both stages.\n",
    "    Uses Stage 2 scores as relevance labels.\n",
    "    \"\"\"\n",
    "    ndcg_results = {k: {'stage1': [], 'stage2': []} for k in k_values}\n",
    "    \n",
    "    for stage1_cands, stage2_cands in zip(stage1_results, stage2_results):\n",
    "        # Use top-100 Stage 2 scores as true relevance\n",
    "        true_relevance = {c['index']: c['stage2_score'] for c in stage2_cands}\n",
    "        \n",
    "        for k in k_values:\n",
    "            # Stage 1 ranking\n",
    "            stage1_topk = stage1_cands[:k]\n",
    "            stage1_relevance = [true_relevance.get(c['index'], 0) for c in stage1_topk]\n",
    "            \n",
    "            # Stage 2 ranking\n",
    "            stage2_topk = stage2_cands[:k]\n",
    "            stage2_relevance = [true_relevance.get(c['index'], 0) for c in stage2_topk]\n",
    "            \n",
    "            # Calculate NDCG (requires at least 2 items)\n",
    "            if len(stage1_relevance) >= 2:\n",
    "                ndcg1 = ndcg_score([stage1_relevance], [list(range(len(stage1_relevance), 0, -1))], k=k)\n",
    "                ndcg2 = ndcg_score([stage2_relevance], [list(range(len(stage2_relevance), 0, -1))], k=k)\n",
    "                \n",
    "                ndcg_results[k]['stage1'].append(ndcg1)\n",
    "                ndcg_results[k]['stage2'].append(ndcg2)\n",
    "    \n",
    "    return ndcg_results\n",
    "\n",
    "print(\"Calculating NDCG improvements...\\n\")\n",
    "ndcg_results = calculate_ndcg_improvement(stage1_results, stage2_results)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"NDCG@K: Stage 1 vs Stage 2\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'K':<5} {'Stage 1':<12} {'Stage 2':<12} {'Improvement':<12} {'Relative Gain'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for k in [5, 10, 20]:\n",
    "    if ndcg_results[k]['stage1']:\n",
    "        ndcg1 = np.mean(ndcg_results[k]['stage1'])\n",
    "        ndcg2 = np.mean(ndcg_results[k]['stage2'])\n",
    "        improvement = ndcg2 - ndcg1\n",
    "        relative_gain = (improvement / ndcg1 * 100) if ndcg1 > 0 else 0\n",
    "        \n",
    "        print(f\"{k:<5} {ndcg1:<12.4f} {ndcg2:<12.4f} {improvement:+.4f}       {relative_gain:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a916a34b",
   "metadata": {},
   "source": [
    "## 10. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6cd99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed timing breakdown\n",
    "print(\"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Benchmark Stage 2 with different batch sizes\n",
    "test_candidates_100 = stage1_results[0][:100]\n",
    "batch_sizes = [8, 16, 32, 64]\n",
    "timing_results = []\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    times = []\n",
    "    for _ in range(3):  # 3 runs\n",
    "        start = time.time()\n",
    "        _ = reranker.rerank(job_descriptions[0], test_candidates_100, batch_size=bs)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    throughput = 100 / avg_time\n",
    "    \n",
    "    timing_results.append({\n",
    "        'batch_size': bs,\n",
    "        'time_seconds': avg_time,\n",
    "        'throughput': throughput\n",
    "    })\n",
    "\n",
    "df_timing = pd.DataFrame(timing_results)\n",
    "\n",
    "print(\"Cross-Encoder Performance (100 candidates):\")\n",
    "print(df_timing.to_string(index=False))\n",
    "\n",
    "print(f\"\\nüí° Optimal batch size: {df_timing.loc[df_timing['time_seconds'].idxmin(), 'batch_size']:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd5e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize timing results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Time vs batch size\n",
    "axes[0].plot(df_timing['batch_size'], df_timing['time_seconds'], marker='o', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[0].set_ylabel('Time (seconds)', fontsize=12)\n",
    "axes[0].set_title('Re-Ranking Time vs Batch Size', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Throughput vs batch size\n",
    "axes[1].plot(df_timing['batch_size'], df_timing['throughput'], marker='s', color='green', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[1].set_ylabel('Throughput (candidates/sec)', fontsize=12)\n",
    "axes[1].set_title('Re-Ranking Throughput vs Batch Size', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS_PATH / 'stage2_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f1b196",
   "metadata": {},
   "source": [
    "## 11. End-to-End Pipeline Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare full pipeline costs\n",
    "print(\"=\"*80)\n",
    "print(\"END-TO-END PIPELINE ANALYSIS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Estimate Stage 1 time (from metadata if available)\n",
    "stage1_meta_path = STAGE1_PATH / 'stage1_metadata.json'\n",
    "if stage1_meta_path.exists():\n",
    "    with open(stage1_meta_path, 'r') as f:\n",
    "        stage1_meta = json.load(f)\n",
    "    stage1_query_time = stage1_meta.get('performance', {}).get('avg_query_time_ms', 10) / 1000\n",
    "else:\n",
    "    stage1_query_time = 0.01  # Assume 10ms\n",
    "\n",
    "# Stage 2 time (100 candidates)\n",
    "stage2_query_time = df_timing.loc[df_timing['batch_size'] == 32, 'time_seconds'].values[0]\n",
    "\n",
    "# Total pipeline\n",
    "total_time = stage1_query_time + stage2_query_time\n",
    "\n",
    "print(f\"Single Query Latency:\")\n",
    "print(f\"  Stage 1 (Retrieval):   {stage1_query_time*1000:7.2f}ms\")\n",
    "print(f\"  Stage 2 (Re-ranking):  {stage2_query_time*1000:7.2f}ms\")\n",
    "print(f\"  {'‚îÄ'*35}\")\n",
    "print(f\"  Total Pipeline:        {total_time*1000:7.2f}ms\")\n",
    "\n",
    "print(f\"\\nThroughput:\")\n",
    "print(f\"  Queries per second: {1/total_time:.2f}\")\n",
    "print(f\"  Queries per minute: {60/total_time:.0f}\")\n",
    "print(f\"  Queries per hour:   {3600/total_time:.0f}\")\n",
    "\n",
    "print(f\"\\nüí° Insights:\")\n",
    "stage2_pct = (stage2_query_time / total_time) * 100\n",
    "print(f\"   - Stage 2 accounts for {stage2_pct:.1f}% of total latency\")\n",
    "print(f\"   - Pipeline suitable for: {'real-time' if total_time < 0.5 else 'batch'} applications\")\n",
    "\n",
    "if stage2_query_time > stage1_query_time * 10:\n",
    "    print(f\"   ‚ö†Ô∏è  Consider GPU acceleration for Stage 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f507bd9",
   "metadata": {},
   "source": [
    "## 12. Case Study: Detailed Ranking Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8621225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed example of re-ranking impact\n",
    "example_jd = job_descriptions[0]\n",
    "example_stage1 = stage1_results[0][:10]\n",
    "example_stage2 = stage2_results[0][:10]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CASE STUDY: Re-Ranking Impact\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nJob Description: {example_jd[:150]}...\\n\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for rank, (s1, s2) in enumerate(zip(example_stage1, example_stage2), 1):\n",
    "    comparison_data.append({\n",
    "        'Rank': rank,\n",
    "        'Stage1_Idx': s1['index'],\n",
    "        'Stage1_Score': f\"{s1['score']:.4f}\",\n",
    "        'Stage2_Idx': s2['index'],\n",
    "        'Stage2_Score': f\"{s2['stage2_score']:.4f}\",\n",
    "        'Moved': '‚úì' if s1['index'] != s2['index'] else '',\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\nTop 10 Comparison:\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "moves = df_comparison['Moved'].value_counts().get('‚úì', 0)\n",
    "print(f\"\\n{moves}/10 positions changed after re-ranking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b98966",
   "metadata": {},
   "source": [
    "## 13. Save Stage 2 Model and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive metadata\n",
    "stage2_metadata = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'creation_date': pd.Timestamp.now().isoformat(),\n",
    "    'device': str(device),\n",
    "    'performance': {\n",
    "        'reranking_time_100_candidates': float(stage2_query_time),\n",
    "        'throughput_candidates_per_sec': float(100 / stage2_query_time),\n",
    "        'optimal_batch_size': int(df_timing.loc[df_timing['time_seconds'].idxmin(), 'batch_size']),\n",
    "    },\n",
    "    'pipeline': {\n",
    "        'stage1_time_ms': float(stage1_query_time * 1000),\n",
    "        'stage2_time_ms': float(stage2_query_time * 1000),\n",
    "        'total_time_ms': float(total_time * 1000),\n",
    "    },\n",
    "    'ablation': {\n",
    "        'avg_overlap_top10': float(df_ablation['overlap_top10'].mean()),\n",
    "        'avg_new_in_top10': float(df_ablation['new_in_top10'].mean()),\n",
    "        'avg_spearman_correlation': float(df_ablation['spearman_correlation'].mean()),\n",
    "    },\n",
    "    'paths': {\n",
    "        'cache': str(cache_path),\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = STAGE2_PATH / 'stage2_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(stage2_metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Stage 2 metadata saved to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091d9927",
   "metadata": {},
   "source": [
    "## 14. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\" \" * 20 + \"STAGE 2: CROSS-ENCODER RE-RANKING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"   - Model: {MODEL_NAME}\")\n",
    "print(f\"   - Job descriptions processed: {len(job_descriptions)}\")\n",
    "print(f\"   - Candidates re-ranked per JD: {len(stage1_results[0])}\")\n",
    "\n",
    "print(\"\\n‚ö° Performance:\")\n",
    "print(f\"   - Re-ranking time (100 candidates): {stage2_query_time:.3f}s\")\n",
    "print(f\"   - Throughput: {100/stage2_query_time:.1f} candidates/sec\")\n",
    "print(f\"   - Full pipeline latency: {total_time*1000:.0f}ms\")\n",
    "\n",
    "print(\"\\nüìà Quality Improvements:\")\n",
    "print(f\"   - Avg ranking changes in top-10: {df_ablation['new_in_top10'].mean():.1f}/10\")\n",
    "print(f\"   - Score correlation (Spearman): {df_ablation['spearman_correlation'].mean():.3f}\")\n",
    "\n",
    "print(\"\\nüíæ Saved Artifacts:\")\n",
    "print(f\"   - Re-ranked results: {cache_path.name}\")\n",
    "print(f\"   - Metadata: {metadata_path.name}\")\n",
    "print(f\"   - Visualizations: stage2_*.png\")\n",
    "\n",
    "print(\"\\nüî¨ Research Insights:\")\n",
    "print(\"   ‚úì Cross-encoders significantly refine bi-encoder rankings\")\n",
    "print(\"   ‚úì Two-stage architecture balances speed and accuracy\")\n",
    "print(\"   ‚úì Stage 2 adds ~100-500ms latency but improves precision\")\n",
    "print(\"   ‚úì Suitable for real-time applications with <1s SLA\")\n",
    "\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "print(\"   - Stage 3: Add LLM judge for explainable scoring\")\n",
    "print(\"   - Fine-tune cross-encoder on domain-specific data\")\n",
    "print(\"   - Implement hybrid scoring (weighted Stage 1 + Stage 2)\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready for Stage 3: LLM Judge Fine-Tuning\")\n",
    "print(\"   üëâ Open: 03_stage3_llm_judge_finetuning.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
