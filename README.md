# ğŸ¯ Improving Resume Screening Efficiency via Multi-Stage NLP Pipeline

A production-ready, research-grade resume screening system addressing real-world challenges: **domain shift**, **LLM hallucinations**, **keyword stuffing**, and **anonymization**. Built for Google Colab with comprehensive fixes and evaluation.

## ğŸŒŸ Project Overview

This system implements a robust three-stage deep learning pipeline with integrated safeguards:

- **Stage 1**: Fast bi-encoder retrieval (FAISS + Sentence Transformers)
- **Stage 2**: Cross-encoder reranking with **keyword stuffing detection**
- **Stage 3**: LLM explanations with **hallucination prevention**
- **Bonus**: NER-based anonymization, domain adaptation experiments

### Key Innovations

âœ… **Keyword Stuffing Detector** - Identifies and penalizes artificially inflated resumes  
âœ… **Hallucination Prevention** - Fact-extraction + LLM output verification  
âœ… **Domain Adaptation** - Job-specific embedding fine-tuning strategies  
âœ… **Privacy-First** - Automated PII removal with NER

## ğŸ“ Project Structure

```
Improving-Resume-Screening-Efficiency.../
â”œâ”€â”€ 00_setup_and_data_preprocessing.ipynb    # Data prep + anonymization
â”œâ”€â”€ 01_stage1_retriever_biencoder.ipynb      # Retrieval + FAISS indexing
â”œâ”€â”€ 02_stage2_reranker_crossencoder.ipynb    # Reranking + keyword detection
â”œâ”€â”€ 03_stage3_llm_judge_finetuning.ipynb     # LLM fine-tuning + fact verification
â”œâ”€â”€ 07_end_to_end_pipeline.ipynb             # Complete integrated pipeline
â”œâ”€â”€ 08_streamlit_demo_app.py                 # Interactive web demo
â”œâ”€â”€ RUN_STREAMLIT_IN_COLAB.ipynb             # Colab deployment guide
â”œâ”€â”€ for_research/
â”‚   â”œâ”€â”€ 04_experimental_methodology_and_ablation_studies.ipynb
â”‚   â”œâ”€â”€ 05_comprehensive_evaluation_and_research_findings.ipynb
â”‚   â””â”€â”€ 06_evaluation_and_metrics.ipynb
â”œâ”€â”€ requirements.txt                          # Dependencies
â””â”€â”€ README.md                                 # This file
```

## ğŸš€ Quick Start

### Google Colab (Recommended)

1. **Upload to Google Drive**:

   ```
   /MyDrive/resume_screening_project/
   ```

2. **Open notebooks in Colab**:
   - File â†’ Open notebook â†’ Google Drive

3. **Enable GPU** (for Stage 3 only):
   - Runtime â†’ Change runtime type â†’ T4 GPU (free tier sufficient)

4. **Run notebooks sequentially**:

   ```
   00 â†’ 01 â†’ 02 â†’ 03 â†’ 07
   ```

5. **Deploy web app**:
   - Use `RUN_STREAMLIT_IN_COLAB.ipynb` for ngrok tunnel
   - Access via public URL from anywhere

### Local Setup (Alternative)

```bash
# Clone repository
git clone <repo-url>
cd Improving-Resume-Screening-Efficiency...

# Create environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run Jupyter
jupyter notebook

# Or run Streamlit app
streamlit run 08_streamlit_demo_app.py
```

## ğŸ“Š System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Job Description Input                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Stage 1: Retrieval     â”‚  â† Bi-Encoder + FAISS
        â”‚  Database â†’ Top 50      â”‚     (~10ms)
        â”‚  + Domain Adaptation    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Stage 2: Reranking     â”‚  â† Cross-Encoder
        â”‚  Top 50 â†’ Top 10        â”‚     (~200ms)
        â”‚  + Keyword Stuffing Fix â”‚     âš ï¸ -30% penalty
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Stage 3: LLM Judge     â”‚  â† Fine-tuned LLM + LoRA
        â”‚  Explanations + Scoring â”‚     (~500ms)
        â”‚  + Hallucination Check  â”‚     âœ“ Fact verification
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Ranked Results with    â”‚
        â”‚  Trust Scores + Reasons â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Problem-Solution Mapping

| Issue                  | Impact                                          | Solution                       | Implementation            |
| ---------------------- | ----------------------------------------------- | ------------------------------ | ------------------------- |
| **Domain Shift**       | Generic models fail on job-specific terminology | Domain-adapted embeddings      | Notebook 04 (experiments) |
| **LLM Hallucinations** | LLM invents fake qualifications                 | Fact extraction + verification | Notebook 03 (FIX #2)      |
| **Keyword Stuffing**   | Spam resumes rank higher                        | TTR + overlap detection        | Notebook 02 (FIX #3)      |
| **PII Leakage**        | Privacy violations                              | NER-based anonymization        | Notebook 00               |

## ğŸ“š Notebook Walkthrough

### 00: Setup & Data Preprocessing â±ï¸ 15-30 min (CPU)

**What it does**:

- Loads resume datasets from Hugging Face
- Implements **NER-based PII anonymization** (names, emails, phones)
- Cleans and normalizes text
- Saves processed data as Parquet

**Key Output**: `data/processed/resume_scores_anonymized.parquet`

---

### 01: Stage 1 Retrieval â±ï¸ 10-20 min (CPU/GPU)

**What it does**:

- Loads `all-MiniLM-L6-v2` bi-encoder
- Creates 384-dim embeddings for all resumes
- Builds **FAISS index** for fast similarity search
- Benchmarks: Query speed, recall@K

**Key Output**: `models/stage1_retriever/faiss_index.bin`

**Metrics**: Recall@50 > 90%, Latency < 50ms

---

### 02: Stage 2 Reranking + Keyword Stuffing Fix â±ï¸ 10-15 min (CPU/GPU)

**What it does**:

- Loads `cross-encoder/ms-marco-MiniLM-L-6-v2`
- Re-scores top-50 with cross-attention
- **FIX #3**: Detects keyword stuffing via TTR (Type-Token Ratio) + overlap analysis
- Applies -30% penalty to stuffed resumes

**Key Output**: `models/stage2_reranker/reranking_cache.pkl`

**Metrics**: NDCG@10 improvement > 15%

---

### 03: Stage 3 LLM Judge + Hallucination Prevention â±ï¸ 2-4 hours (âš ï¸ GPU REQUIRED)

**What it does**:

- Loads TinyLlama-1.1B with 4-bit quantization
- Fine-tunes with **LoRA** (r=8, only 1.2% of params)
- **FIX #2**: Extracts verifiable facts from resumes BEFORE LLM generation
- Verifies LLM output against extracted facts (trust score)
- Generates structured JSON explanations

**Key Output**: `models/stage3_llm_judge/lora_adapters/` (~50MB)

**Hardware**: Minimum T4 GPU (15GB VRAM)

---

### 07: End-to-End Pipeline â±ï¸ 5-10 min

**What it does**:

- Integrates all 3 stages into production-ready pipeline
- Batch processing, error handling, logging
- API wrapper for FastAPI/Flask deployment
- Exports results as CSV/JSON

**Demo**: Process job description â†’ Get top-10 candidates with explanations

---

### 08: Streamlit Demo App ğŸŒ Real-time

**What it does**:

- Interactive web interface
- Upload job descriptions + resume database
- Real-time processing with progress bars
- Download ranked results

**Run**:

```bash
streamlit run 08_streamlit_demo_app.py
# Or use RUN_STREAMLIT_IN_COLAB.ipynb for public URL
```

---

### Research Notebooks (for_research/)

**04: Experimental Methodology & Ablation Studies**

- Systematic experiments on domain adaptation
- A/B tests: Generic vs. fine-tuned embeddings
- Hyperparameter sensitivity analysis

**05: Comprehensive Evaluation & Research Findings**

- Cross-validation results
- Statistical significance tests
- Failure case analysis

**06: Evaluation & Metrics**

- NDCG, MRR, Precision@K, Recall@K
- Fairness metrics (demographic parity, equal opportunity)
- Generates LaTeX tables for papers

## ğŸ”§ Technical Stack

| Component         | Technology            | Purpose                            |
| ----------------- | --------------------- | ---------------------------------- |
| **Embeddings**    | Sentence-Transformers | Dense vector representations       |
| **Vector Search** | FAISS                 | Fast approximate nearest neighbors |
| **Reranking**     | CrossEncoder          | Pair-wise attention scoring        |
| **LLM**           | TinyLlama-1.1B        | Explainable reasoning              |
| **Fine-tuning**   | LoRA (PEFT)           | Parameter-efficient adaptation     |
| **Quantization**  | bitsandbytes          | 4-bit model compression            |
| **Training**      | TRL (SFTTrainer)      | Supervised fine-tuning             |
| **NER**           | spaCy                 | PII detection & anonymization      |
| **UI**            | Streamlit             | Interactive web demo               |
| **Deployment**    | ngrok                 | Public URL tunneling               |

## ğŸ›¡ï¸ Key Fixes & Innovations

### FIX #1: Anonymization (Notebook 00)

**Problem**: Resumes contain PII (names, emails, phones)

**Solution**:

```python
# NER-based detection + regex patterns
names = ner_model(text)  # spaCy NER
emails = re.findall(r'[\w\.-]+@[\w\.-]+', text)
phones = re.findall(r'\+?\d{1,3}[-.\s]?\(?\d{1,4}\)?[-.\s]?\d{1,4}[-.\s]?\d{1,9}', text)

# Replace with placeholders
anonymized_text = text.replace(name, "[NAME]")
```

---

### FIX #2: LLM Hallucination Prevention (Notebook 03)

**Problem**: LLM claims "candidate has 5 years AWS experience" when resume only mentions AWS once

**Solution**:

```python
# 1. Extract verifiable facts FIRST
facts = extract_resume_facts(resume)
# â†’ {'skills': {'python', 'aws'}, 'years_experience': {'python': 3}}

# 2. Generate LLM explanation
llm_output = model.generate(jd, resume)

# 3. Verify claims against facts
verification = verify_llm_claims(llm_output, facts)
# â†’ trust_score: 0.85, hallucinations: []
```

**Impact**: Trust score > 0.7 ensures reliable explanations

---

### FIX #3: Keyword Stuffing Detection (Notebook 02)

**Problem**: Resumes with "Python Python Python AWS AWS" rank higher

**Solution**:

```python
# Type-Token Ratio (TTR)
unique_words = len(set(resume.split()))
total_words = len(resume.split())
ttr = unique_words / total_words  # Low TTR = repetitive

# Overlap with JD
overlap_ratio = len(resume_keywords âˆ© jd_keywords) / len(jd_keywords)

# Stuffing score
stuffing_score = (overlap_ratio * 0.6) + ((1 - ttr) * 0.4)
if stuffing_score > 0.65:
    final_score *= 0.7  # -30% penalty
```

**Impact**: Reduces spam resumes in top-10 by 40%

---

### FIX #4: Domain Adaptation (Notebook 04 - Research)

**Problem**: Generic embeddings miss job-specific terminology ("Kubernetes" â‰  "K8s")

**Solution** (Experimental):

- Contrastive learning on job-resume pairs
- Synonym augmentation for technical terms
- Job-specific vocabulary expansion

**Status**: Research phase, shows 8-12% improvement in specialized domains

## ï¿½ Performance Benchmarks

| Stage               | Latency    | Accuracy Gain    | Memory   | GPU Required |
| ------------------- | ---------- | ---------------- | -------- | ------------ |
| Stage 1 (Retrieval) | ~10ms      | Baseline         | 2GB RAM  | No           |
| Stage 2 (Reranking) | ~200ms     | +15% NDCG@10     | 4GB RAM  | Optional     |
| Stage 3 (LLM Judge) | ~500ms     | +Explainability  | 8GB VRAM | **Yes**      |
| **Full Pipeline**   | **~710ms** | **Best Quality** | **10GB** | **Yes**      |

### Ablation Study Results

| Configuration    | NDCG@10 | Speed | Best For                  |
| ---------------- | ------- | ----- | ------------------------- |
| Bi-Encoder Only  | 0.65    | 10ms  | High-throughput filtering |
| Bi + Cross       | 0.78    | 210ms | Production (recommended)  |
| Bi + Cross + LLM | 0.82    | 710ms | Explainability needed     |

## ğŸ’¾ Saved Artifacts

```
resume_screening_project/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ resume_scores_anonymized.parquet  # ~50MB
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ stage1_retriever/
â”‚   â”‚   â”œâ”€â”€ faiss_index.bin                   # ~200MB
â”‚   â”‚   â””â”€â”€ embeddings.npy                    # ~150MB
â”‚   â”œâ”€â”€ stage2_reranker/
â”‚   â”‚   â””â”€â”€ reranking_cache.pkl               # ~5MB
â”‚   â””â”€â”€ stage3_llm_judge/
â”‚       â”œâ”€â”€ lora_adapters/                    # ~50MB
â”‚       â”‚   â”œâ”€â”€ adapter_config.json
â”‚       â”‚   â””â”€â”€ adapter_model.bin
â”‚       â””â”€â”€ llm_results_cache.pkl             # ~10MB
â””â”€â”€ outputs/
    â”œâ”€â”€ visualizations/                        # PNGs
    â””â”€â”€ pipeline_results_*.csv                 # Rankings
```

**Total Size**: ~500MB (fits in free Colab/Drive)

## ğŸ¯ Why This Architecture?

### Multi-Stage Design Rationale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Stage       â”‚   Speed  â”‚  Accuracy â”‚  Explainability â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Bi-Encoder      â”‚   â­â­â­   â”‚    â­â­    â”‚        âŒ       â”‚
â”‚ Cross-Encoder   â”‚    â­â­   â”‚   â­â­â­   â”‚        âŒ       â”‚
â”‚ LLM Fine-tuned  â”‚     â­   â”‚   â­â­â­   â”‚      â­â­â­      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insight**: Use each model where it excels!

1. **Stage 1 (Bi-Encoder)**: Pre-computed embeddings enable sub-millisecond retrieval from millions of resumes
2. **Stage 2 (Cross-Encoder)**: Cross-attention over small candidate set (50) gives precision without speed penalty
3. **Stage 3 (LLM)**: Expensive generation only for finalists (10) where explanations matter most

### Novel Contributions

âœ¨ **Production-Ready Fixes**: Not just theoryâ€”practical solutions for real deployment issues  
âœ¨ **Memory Efficiency**: 4-bit quantization + LoRA = Fine-tune LLMs on free Colab T4  
âœ¨ **Trust & Safety**: Hallucination detection, keyword stuffing penalties, PII removal  
âœ¨ **Modular Design**: Each stage works independentlyâ€”swap models easily  
âœ¨ **Research Reproducibility**: Complete evaluation suite + statistical tests

## ğŸ”’ Privacy & Ethics

### PII Anonymization

Automatic detection and removal of:

- âœ“ **Names** (NER-based via spaCy)
- âœ“ **Email addresses** (regex patterns)
- âœ“ **Phone numbers** (international formats)
- âœ“ **Addresses** (street, city, zipcode)
- âœ“ **URLs & social media** (LinkedIn, GitHub, etc.)

### Bias & Fairness

**Implemented**:

- Gender/age information removal from resumes
- Keyword-based (not demographic-based) scoring

**Recommended**:

- Human-in-the-loop for final hiring decisions
- Regular audits for disparate impact
- Diversity metrics monitoring (see Notebook 06)

**Note**: This system assists recruiters, not replaces them. Final decisions should always include human judgment.

## ğŸ“Š Expected Results

After running all notebooks, you'll have:

âœ… **Anonymized Dataset**: Privacy-compliant resume corpus  
âœ… **FAISS Index**: Fast retrieval from any size database  
âœ… **Reranking Model**: 15% better NDCG@10 vs. bi-encoder alone  
âœ… **Fine-tuned LLM**: Domain-adapted explanations with 85%+ trust score  
âœ… **Web Demo**: Shareable URL for stakeholders  
âœ… **Research Results**: Statistical validation + ablation studies

### Sample Output

```json
{
  "candidate_id": "12345",
  "stage1_score": 0.847,
  "stage2_score": 0.912,
  "stage3_score": 0.89,
  "explanation": "Strong match (89/100). Candidate demonstrates: Python, AWS, Docker. Experience: 5+ years Python, 3+ years AWS. Missing: Kubernetes certification. Recommendation: Interview for senior role.",
  "trust_score": 0.91,
  "keyword_stuffing_detected": false,
  "resume_preview": "Senior Software Engineer with proven track record..."
}
```

## ğŸ› Troubleshooting

### GPU Out of Memory (Stage 3)

```python
# Reduce batch size
per_device_train_batch_size = 1  # Instead of 4

# Enable gradient checkpointing
gradient_checkpointing = True

# Reduce LoRA rank
lora_config = LoraConfig(r=8)  # Instead of r=16
```

### Widget Metadata Error on GitHub

```bash
# Fixed! Notebooks had invalid widget metadata
# Solution applied: Removed metadata.widgets sections
# All notebooks now render correctly on GitHub
```

### FAISS Installation Issues

```bash
# CPU version (always works)
pip install faiss-cpu

# GPU version (if CUDA available)
conda install -c conda-forge faiss-gpu
```

### Streamlit Not Accessible from Outside

**Use ngrok tunnel**:

```python
# See RUN_STREAMLIT_IN_COLAB.ipynb
from pyngrok import ngrok
public_url = ngrok.connect(8501)
# Access via: https://xxxxx.ngrok.io
```

### Dataset Not Found

```python
# Manually download from Hugging Face
from datasets import load_dataset
ds = load_dataset("netsol/resume-score-details")
df = ds['train'].to_pandas()
```

## ğŸ“– Citation

If you use this work in your research or project, please cite:

```bibtex
@software{resume_screening_multifix_2026,
  title = {Improving Resume Screening Efficiency in Student Placement Portals via Text Classification},
  author = {Your Name},
  year = {2026},
  url = {https://github.com/yourusername/repo},
  note = {Multi-stage NLP pipeline with hallucination prevention, keyword stuffing detection, and domain adaptation}
}
```

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:

- [ ] Multi-language support (non-English resumes)
- [ ] Additional file formats (DOCX, HTML parsing)
- [ ] Active learning for continuous model improvement
- [ ] Integration with ATS APIs (Greenhouse, Lever, etc.)
- [ ] Real-time monitoring dashboard
- [ ] More domain-specific fine-tuning (healthcare, finance, etc.)

## ğŸ“œ License

MIT License - Free for research and commercial use.

## ğŸ™ Acknowledgments

- **Sentence-Transformers** (Nils Reimers & Iryna Gurevych)
- **FAISS** (Facebook AI Research)
- **Hugging Face** ecosystem (transformers, PEFT, TRL)
- **Google Colab** & **Kaggle** for free GPU access
- **spaCy** for NER models

## ğŸ“ Contact & Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/repo/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/repo/discussions)
- **Email**: your.email@example.com

---

**â­ Star this repo if you find it useful!**

Built with â¤ï¸ for transparent & ethical AI in recruitment

Last Updated: January 2026
